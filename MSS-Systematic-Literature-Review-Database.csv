paper_id;Reference;abstract_summary_tr;Title;Year;paper_type;main_contribution;problem;separation_task;channel_configuration;training_dataset;test_dataset;real_or_synthetic_mix;data_augmentation;preprocessing;visualization_type;learning_type;architecture_family;specific_models;loss_function;evaluation_metrics;separation_performance
1;Araki, S., Ito, N., Haeb-Umbach, R., Wichern, G., Wang, Z. Q., & Mitsufuji, Y. (2025, April). 30+ years of source separation research: Achievements and future challenges. In ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 1-5). IEEE.;"Bu çal??ma, akustik sinyallerin kaynak ayr??t?r?lmas? (Source Separation – SS) alan?n?n yakla??k otuz y?ll?k geli?imini kapsaml? bir ?ekilde ele alan bir derleme makalesidir. 1990’lar?n ortas?nda ortaya ç?kan konu?ma, ses ve müzik kaynak ayr??t?rma ara?t?rmalar?n?n tarihsel evrimi incelenmi?; hem tek kanall? hem de çok kanall? yakla??mlar sistematik biçimde de?erlendirilmi?tir. Makale, yaln?zca yöntemsel ilerlemeleri de?il, ayn? zamanda alan?n olgunla?mas?n? sa?layan bilimsel de?erlendirme kültürünü de ele alarak; ortak veri setleri, performans ölçütleri ve topluluk temelli yar??malar?n (challenge) rolünü vurgulamaktad?r. Son olarak, güncel e?ilimler ve gelecekteki ara?t?rma yönleri tart???larak, kaynak ayr??t?rma alan?n?n mevcut durumu ve potansiyel geli?im alanlar? bütüncül bir bak?? aç?s?yla sunulmaktad?r.";30+ Years of Source Separation Research: Achievements and Future Challenges;2025;Review;A comprehensive survey summarizing more than 30 years of research in speech, audio, and music source separation, covering model-based, deep learning-based, and hybrid approaches, as well as benchmarks, datasets, evaluation metrics, and future research directions;Separating individual acoustic source signals from one or more mixtures under blind or semi-blind settings;Speech separation / audio source separation / music source separation;Single-channel and Multi-channel;-;-;-;-;STFT-based time-frequency representation (general problem formulation);Time-Frequency domain;Supervised + Unsupervised (surveyed);Model-based + Deep Learning + Hybrid;ICA / IVA / ILRMA / NMF / TF masking / Beamforming / Conv-TasNet / DPRNN / Transformer-based models;-;SDR / SI-SDR / SIR / SAR / STOI / PESQ / WER / MOS;-
2;Lu, W. T., Wang, J. C., Kong, Q., & Hung, Y. N. (2024, April). Music source separation with band-split rope transformer. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 481-485). IEEE.;Bu çal??ma, müzik kaynak ay?rma için frekans alan?nda çal??an Transformer tabanl? bir mimari olan BS-RoFormer’i önermektedir. Model, karma??k spektrogram? band-split modülü ile alt frekans bantlar?na ay?rmakta ve hiyerar?ik Transformer katmanlar? ile hem bant içi hem de bantlar aras? ili?kileri birlikte modellemektedir. Bu yönüyle, tek-sekans Transformer kullanan yakla??mlardan ayr?lmaktad?r. Çal??man?n öne ç?kan bir di?er katk?s?, Rotary Position Embedding (RoPE) kullan?m?n?n MSS ba?lam?nda etkin biçimde gösterilmesidir. Çok bantl? maske tahmini ile birle?tirilen bu yap?, hem mimari verimlilik hem de performans aç?s?ndan güçlü sonuçlar sunmaktad?r. Ek veri olmadan state-of-the-art performansa ula?mas? ve yar??ma birincili?i elde etmesi, yöntemin genellenebilirli?ini ve literatüre katk?s?n? aç?kça ortaya koymaktad?r.;MUSIC SOURCE SEPARATION WITH BAND-SPLIT ROPE TRANSFORMER;2024;Research (ICASSP paper);"Proposes BS-RoFormer, a frequency-domain music source separation model using a band-split front-end and hierarchical interleaved Transformers with Rotary Position Embedding (RoPE) for multi-band complex mask estimation; achieves state-of-the-art SDR and ranked 1st in SDX’23 music separation track";"Music source separation (4-stem) from stereo recordings is challenging due to complex signals and multiple sources; improve frequency-domain modeling efficiency and separation quality by leveraging band-wise priors and Transformer sequence modeling with stable positional encoding";4-stem MSS (vocals, bass, drums, other) via complex ideal ratio mask (cIRM) estimation in STFT domain;Stereo (2-channel);"MUSDB18HQ + In-House (500 songs; 450 train, 50 val) for SDX’23 system; ablations use MUSDB18HQ only";"MUSDB18HQ test/eval set (50 songs); SDX’23 organizer private test set for leaderboard C";Real (recorded multitrack stems) + synthetic mixtures via random mixing of stems (dynamic pool);"Random gain ±3 dB per stem; 10% chance a stem replaced with silence; random mixing of stems possibly from different songs; random 8-second cropping with loudness > -50 dB; dynamic segment pool";"STFT/iSTFT; complex spectrogram input; band-split into 62 non-overlapping subbands; segment enframe/deframe (TC or OA); RMSNorm; mixed precision (STFT/iSTFT FP32, others FP16); EMA; flash attention; checkpointing";Time-Frequency (complex spectrogram);Supervised;Transformer (hierarchical/interleaved);"BS-RoFormer (Band-Split RoPE Transformer); baseline comparisons include BSRNN, HTDemucs, etc.";Time-domain MAE + multi-resolution complex spectrogram MAE (S=5 multi-resolution STFTs);"SDR (museval); median SDR reporting (per 1-second chunks)";"9.80 dB average SDR on MUSDB18HQ without extra data (BS-RoFormer L=6, OA); 9.97 dB mean global SDR on SDX’23 leaderboard C final (private set); 11.99 dB avg SDR for SDX’23 submission model with extra data (L=12, OA)"
3;Mariani, G., Tallini, I., Postolache, E., Mancusi, M., Cosmo, L., & Rodolà, E. (2023). Multi-source diffusion models for simultaneous music generation and separation. arXiv preprint arXiv:2302.02257.;"Bu çal??ma, difüzyon tabanl? tek bir üretici model ile hem müzik üretimi hem de müzik kaynak ay?rma görevlerini birlikte ele almas?yla öne ç?kmaktad?r. Model, ayn? ba?lam? payla?an kaynaklar?n ortak olas?l?k da??l?m?n? ö?renerek yaln?zca klasik ay?rma ve kar???m üretimi görevlerini de?il, ayn? zamanda k?smi üretim (source imputation) senaryosunu da mümkün k?lmaktad?r; yani baz? enstrümanlar verilmi?ken di?erlerinin tutarl? biçimde üretilmesini sa?lamaktad?r. Ayr?ca kaynak ay?rma için Dirac olas?l?k fonksiyonlar?na dayal? yeni bir ç?kar?m yöntemi önerilmektedir. Slakh2100 veri kümesi üzerinde e?itilen model, üretim taraf?nda niteliksel olarak tutarl? sonuçlar, ay?rma taraf?nda ise rekabetçi nicel performans sergilemektedir. Bu yönleriyle çal??ma, üretim ve ay?rmay? tek bir çat? alt?nda birle?tiren ilk yakla??m olarak genel amaçl? ses modellerine do?ru önemli bir ad?m sunmaktad?r.";MULTI-SOURCE DIFFUSION MODELS FOR SIMULTANEOUS MUSIC GENERATION AND SEPARATION;2024;Conference (ICLR);Introduces a Multi-Source Diffusion Model that learns the joint distribution of contextual musical stems, enabling total generation, partial generation (source imputation), and source separation within a single unified model, and proposes a novel Dirac-based likelihood for posterior score estimation in separation;Joint modeling of multiple dependent musical sources to enable both generation and separation without conditioning architecture on mixtures;"Music source separation; music mixture generation; partial generation (source imputation)";"Multi-source (4 stems); mono waveform per stem (stacked channels)";"Slakh2100 (145h; 4 most abundant classes: Bass, Drums, Guitar, Piano)";Slakh2100 test set;Real (rendered multi-track stems);NG;"Waveform domain; Gaussian forward diffusion process; 22kHz resampling; chunking (~12s); noise schedule with ?(t); ODE-based sampling";Waveform (time-domain);"Supervised (score-matching diffusion); also weakly-supervised variant with independent priors";"Diffusion model (Score-based; U-Net backbone)";"MSDM (Multi-Source Diffusion Model); ISDM; MSDM Dirac; MSDM Gaussian";Denoising score-matching loss (MSE between predicted and true denoised signal);"SI-SDRI; FAD; sub-FAD; subjective listening scores (quality; coherence; density)";"Up to 17.27 dB SI-SDRI (ISDM Dirac with correction); MSDM Dirac comparable to Demucs baseline; competitive with state-of-the-art regressor models on Slakh2100"
4;Fabbro, G., Uhlich, S., Lai, C. H., Choi, W., Martínez-Ramírez, M., Liao, W., ... & Mitsufuji, Y. (2023). The Sound Demixing Challenge 2023$\unicode {x2013} $ Music Demixing Track. arXiv preprint arXiv:2308.06979.;Bu çal??ma, Sound Demixing Challenge 2023 (SDX’23) kapsam?ndaki Music Demixing (MDX) yar??ma parkurunu özetlemekte ve özellikle gürültülü ve hatal? etiketler içeren e?itim verileri alt?nda müzik kaynak ay?rma (robust MSS) problemine odaklanmaktad?r. Makalede, MSS veri kümelerinde ortaya ç?kabilecek etiket hatalar? ve kaynak s?z?nt?lar? (bleeding) sistematik olarak tan?mlanmakta ve bu hatalar? simüle eden iki yeni veri kümesi sunulmaktad?r. Ayr?ca yar??mada en yüksek performans? elde eden yakla??mlar incelenmekte ve önceki yar??ma (MDX 2021) ile do?rudan kar??la?t?rma yap?larak, en iyi sistemin 1.6 dB’den fazla SDR iyile?mesi sa?lad??? gösterilmektedir. Çal??ma, yaln?zca nesnel metriklere de?il, ayn? zamanda profesyonel müzisyen ve prodüserlerle yap?lan dinleme testlerine de yer vererek alg?sal kaliteyi de?erlendirmekte ve son olarak yar??ma organizasyonu ile gelecekteki edisyonlara yönelik ç?kar?mlar sunmaktad?r.;D3Net: Densely Connected Multi-Dilated DenseNet for Music Source Separation;2021;Conference (ICASSP);Proposes D3Net, a densely connected multi-dilated DenseNet architecture that enlarges the receptive field while maintaining parameter efficiency for music source separation, achieving improved SDR compared to prior convolutional models;Improving music source separation performance by better capturing multi-scale temporal and spectral context with efficient convolutional modeling;Music source separation (4 stems: vocals, drums, bass, other);Stereo (2-channel);MUSDB18;MUSDB18 test set;Real (professionally recorded multitrack stems);Data augmentation via random mixing and segment cropping (NG exact details);"STFT-based magnitude spectrogram input; U-Net style encoder-decoder with dense and dilated convolutions";Time-Frequency (magnitude spectrogram);Supervised;Convolutional Neural Network (DenseNet-based U-Net);D3Net;L1 loss on magnitude spectrogram (NG if exact formulation unspecified);SDR (BSS Eval / museval);Improved average SDR over prior CNN-based baselines on MUSDB18 (exact value NG)
5;Sawata, R., Takahashi, N., Uhlich, S., Takahashi, S., & Mitsufuji, Y. (2024). The whole is greater than the sum of its parts: improving music source separation by bridging networks. EURASIP Journal on Audio, Speech, and Music Processing, 2024(1), 39.;"Bu çal??ma, derin ö?renme tabanl? müzik kaynak ay?rma sistemlerinin performans?n? hesaplama maliyetini neredeyse art?rmadan iyile?tirmeyi amaçlayan X-scheme (crossing scheme) adl? bir yöntemi sunmaktad?r. X-scheme; çok-alanl? kay?p fonksiyonu (multi-domain loss), farkl? enstrümanlara ait a?lar aras?nda bilgi payla??m? sa?layan bridging (köprüleme) i?lemi ve bu yap?y? destekleyen combination loss bile?enlerinden olu?maktad?r. Yöntem, zaman ve frekans alan? temsillerini birlikte kullanarak ve kaynaklar aras? ili?kileri e?itim a?amas?nda modele dahil ederek ay?rma kalitesini art?rmaktad?r. Önerilen yakla??m?n en önemli özelli?i, yaln?zca e?itim s?ras?nda uygulanmas?, ç?kar?m a?amas?n? ve parametre say?s?n? etkilememesidir. Deneysel sonuçlar, Open-Unmix, D3Net ve Conv-TasNet gibi farkl? MSS mimarilerinin X-scheme ile geni?letildi?inde tutarl? performans kazan?mlar? elde etti?ini göstermekte ve yöntemin genel, mimariden ba??ms?z bir iyile?tirme stratejisi oldu?unu ortaya koymaktad?r.";Hybrid Demucs: A Hybrid Spectrogram and Waveform Source Separation Model;2023;Conference (ICASSP);Proposes Hybrid Demucs, a model that combines waveform-domain and spectrogram-domain U-Net architectures to leverage both time-domain and frequency-domain representations for improved music source separation;Improving music source separation by jointly modeling waveform and spectrogram features to overcome limitations of purely time-domain or frequency-domain approaches;Music source separation (4 stems: vocals, drums, bass, other);Stereo (2-channel);MUSDB18-HQ + additional proprietary and public datasets (as part of training strategy);MUSDB18-HQ test set;Real (professionally recorded multitrack stems) + synthetic mixtures during training;Data augmentation including random remixing of stems, pitch shifting, time stretching, and random segment cropping;"STFT for spectrogram branch; raw waveform input for time-domain branch; hybrid encoder-decoder with cross-domain fusion";Waveform + Time-Frequency (Hybrid);Supervised;Hybrid CNN-based U-Net (waveform + spectrogram);Hybrid Demucs;L1 loss on waveform + multi-resolution STFT loss;SDR (museval);State-of-the-art SDR on MUSDB18-HQ at time of publication (exact value NG)
6;Rouard, S., Massa, F., & Défossez, A. (2023, June). Hybrid transformers for music source separation. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 1-5). IEEE.;"Bu çal??ma, müzik kaynak ay?rma probleminde uzun menzilli ba?lamsal bilginin önemini ara?t?rmakta ve bu amaçla Hybrid Transformer Demucs (HT Demucs) adl? yeni bir mimari önermektedir. HT Demucs, zaman ve frekans alanlar?n? birlikte kullanan hibrit bir U-Net yap?s?n?, iç katmanlarda çapraz-alan Transformer Encoder ile güçlendirmekte; böylece hem alan içi self-attention hem de alanlar aras? cross-attention ile uzun süreli ba??ml?l?klar? modelleyebilmektedir. Deneyler, Transformer bile?enlerinin küçük veri rejimlerinde s?n?rl? fayda sa?lad???n?, ancak geni? ölçekli ek e?itim verisi kullan?ld???nda modelin Hybrid Demucs’e k?yasla anlaml? performans art??? sa?lad???n? göstermektedir. Seyrek attention mekanizmalar? ve kaynak ba??na ince ayar ile HT Demucs, ek veriyle MUSDB üzerinde state-of-the-art sonuçlara ula?arak uzun ba?lam bilgisinin MSS için etkili oldu?unu ortaya koymaktad?r.";Hybrid Transformers for Music Source Separation;2023;Conference (ICASSP);Proposes Hybrid Transformer Demucs (HT Demucs), a cross-domain Transformer-based extension of Hybrid Demucs that replaces inner convolutional layers with a cross-domain Transformer encoder using self-attention and cross-attention between waveform and spectrogram branches, achieving state-of-the-art SDR with extra data;Improving music source separation by leveraging long-range contextual modeling through Transformers in both temporal and spectral domains;Music source separation (4 stems: drums, bass, vocals, other);Stereo (2-channel);MUSDB18-HQ (87 training songs) + 800 curated internal songs (selected from 3500);MUSDB18-HQ test set;Real multitrack recordings + curated internal dataset;"Remixing; repitching; tempo stretching; segment cropping; per-source fine-tuning without augmentation";"STFT for spectral branch; waveform input for temporal branch; chunk-based training (3.4–12.2 sec); overlap-add at inference; cross-domain Transformer encoder; sparse attention with LSH for long context";Waveform + Time-Frequency (Hybrid);Supervised;Hybrid CNN + Transformer (Bi-U-Net with cross-domain Transformer encoder);"HT Demucs; Sparse HT Demucs (fine-tuned variant)";"L1 loss on waveform; Adam optimizer; EMA; gradient clipping during fine-tuning";"SDR (SiSEC18 definition; median over 1-sec chunks); Real Time Factor (RTF)";"9.20 dB SDR on MUSDB-HQ test set with 800 extra songs and sparse attention + fine-tuning; +0.45 dB over Hybrid Demucs trained on same data"
7;Luo, Y., & Yu, J. (2023). Music source separation with band-split RNN. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 31, 1893-1901.;Bu çal??ma, müzik kaynak ay?rma alan?nda müzi?e özgü frekans yap?s?n? do?rudan modelleyen bir yakla??m olarak Band-Split RNN (BSRNN) mimarisini önermektedir. Model, giri? spektrogram?n? önceden tan?ml? frekans alt bantlar?na ay?rmakta ve bu bantlar üzerinde bant-seviyesinde ve zaman-seviyesinde ard???k (interleaved) modelleme yapmaktad?r. Alt bant geni?liklerinin, hedef enstrümanlar?n özelliklerine göre uzman bilgisiyle ayarlanabilmesi, yöntemin ay?rt edici yönlerinden biridir. Ayr?ca çal??ma, etiketlenmemi? verilerden faydalanmak için yar? denetimli bir ince ayar (fine-tuning) süreci tan?mlamakta ve bu sürecin performans? daha da art?rd???n? göstermektedir. BSRNN, yaln?zca MUSDB18-HQ ile e?itildi?inde dahi MDX 2021 yar??mas?ndaki önde gelen modelleri geride b?rakarak, frekans-bant temelli modellemenin MSS için güçlü bir paradigma oldu?unu ortaya koymaktad?r.;Music Source Separation With Band-Split RNN;2023;Journal (IEEE/ACM TASLP);Proposes Band-Split RNN (BSRNN), a frequency-domain music source separation model that explicitly splits spectrograms into source-specific subbands and performs interleaved band-level and sequence-level modeling, along with a semi-supervised self-boosting finetuning pipeline;Improving music source separation by incorporating prior knowledge of instrument-specific frequency characteristics and better modeling high-sample-rate signals;Music source separation (4 stems: vocals, bass, drums, other) treated as source extraction (one target at a time);Stereo (2-channel);MUSDB18-HQ (supervised training);MUSDB18-HQ and MUSDB18 test sets;Real multitrack recordings + additional private unlabeled dataset (1750 songs) for semi-supervised finetuning;"On-the-fly remixing; energy rescaling (?10 to 10 dB); random source dropping (p=0.1); semi-supervised pseudo-label generation and clean segment detection";"STFT (window 2048, hop 512, Hann window); complex spectrogram input; band splitting into predefined subbands; BLSTM residual modeling; overlap-add at inference";Time-Frequency (complex spectrogram);Supervised + Semi-supervised finetuning;"RNN-based (BLSTM; dual-path style band and sequence modeling)";BSRNN;Frequency-domain MAE (real + imaginary) + time-domain MAE on reconstructed waveform;"uSDR (MDX metric); cSDR (SiSEC bss_eval median over 1-sec chunks)";"Outperforms prior state-of-the-art models on vocals, drums, and other tracks on MUSDB18-HQ; semi-supervised finetuning further improves performance (~1 dB gain on bass and drums cSDR); achieves top-ranking performance in MDX Challenge setting"
8;Plaja-Roglans, G., Miron, M., Shankar, A., & Serra, X. (2023). Carnatic singing voice separation using cold diffusion on training data with bleeding.;"Bu çal??ma, etiketli ve temiz kaynak verisinin s?n?rl? oldu?u durumlarda ?ark? sesi ay?rma problemini ele almaktad?r. Yöntem, kaynak s?z?nt?s? (bleeding) içeren yer-gerçekli (ground-truth) verilerle e?itilebilen frekans alan?nda bir cold diffusion modeli önermektedir. Model, kar???m? ad?m ad?m hedef vokale dönü?türürken, her ad?mda spektrogramdaki de?i?imleri izler; son a?amada ise bu dönü?üm süreci boyunca benzer evrim gösteren frekans-zaman noktalar?n? kümeleyerek ay?rma maskeleri olu?turur. Yaln?zca bleeding içeren veri setlerinin bulundu?u Carnatic müzi?i senaryosunda test edilen yakla??m, Bat? müzi?iyle e?itilmi? genel modellerin aksine bu repertuvara daha uygun sonuçlar üretmekte; Spleeter’a k?yasla giri?imi daha iyi bast?r?rken sinyal bozulmas? aç?s?ndan rekabetçi performans sunmaktad?r. Aç?k kaynak kod payla??m? da çal??man?n pratik katk?lar?ndand?r.";Carnatic Singing Voice Separation Using Cold Diffusion on Training Data with Bleeding;2023;Conference (ISMIR);Proposes a singing voice separation system that can be trained only with mixtures and target sources containing bleeding by leveraging a cold diffusion process in the spectrogram domain followed by unsupervised clustering to build final separation masks;Singing voice separation in Carnatic music when clean isolated ground-truth sources are unavailable and only multitrack recordings with source bleeding exist;Singing voice separation (vocal extraction);Single-channel (mono mixture processed via magnitude spectrogram);Saraga dataset (?36 hours, multitrack with bleeding);Custom recorded Carnatic test set (?2 hours, bleeding-free stems for evaluation) + additional perceptual test data from Saraga and Dunya;Real recordings with source bleeding (training) + real clean recordings for evaluation;NG;"STFT (window 1024, hop 256, 22.05 kHz); magnitude spectrogram; cold diffusion process with T=8 steps; phase reused from mixture for reconstruction";Time-Frequency (magnitude spectrogram);Supervised (cold diffusion training) + Unsupervised (K-means clustering mask estimation);U-Net-based diffusion-inspired architecture;Cold Diffusion U-Net for mask prediction + K-means clustering for mask refinement;L2 loss between predicted and true reverse diffusion step (mask-based objective);"SDR; SIR; SAR; MOS (MUSHRA-based perceptual evaluation)";"Improves SIR over baseline U-Net and Spleeter (up to +7.14 dB SIR improvement vs no clustering); competitive perceptual vocal quality despite lower SAR; better interference removal in Carnatic domain"
9;Pereira, I., Araújo, F., Korzeniowski, F., & Vogl, R. (2023). Moisesdb: A dataset for source separation beyond 4-stems. arXiv preprint arXiv:2307.15913.;"Bu çal??ma, müzik kaynak ay?rma ara?t?rmalar? için MoisesDB adl? yeni ve kapsaml? bir veri kümesini tan?tmaktad?r. MoisesDB, 45 sanatç?ya ait 240 parça ve 12 farkl? müzik türünü kapsamakta; her parça için kaynaklar iki seviyeli hiyerar?ik bir stem taksonomisi alt?nda sunulmaktad?r. Bu yap?, geleneksel dört-stem (vokal, bas, davul, di?er) s?n?r?n?n ötesine geçerek daha ayr?nt?l? (fine-grained) kaynak ay?rma çal??malar?n? mümkün k?lmaktad?r. Çal??ma, veri kümesinin kullan?m?n? kolayla?t?rmak amac?yla Python tabanl? bir araç kütüphanesi ve detayl? dokümantasyon sa?lamaktad?r. Ayr?ca farkl? ay?rma ayr?nt? seviyeleri (4, 5 ve 6 stem) için aç?k kaynak modellerle elde edilen temel (baseline) sonuçlar sunularak, MoisesDB’nin MSS literatüründeki konumu ve potansiyeli ortaya konmaktad?r.";MoisesDB: A Dataset for Source Separation beyond 4-Stems;2023;Conference (ISMIR);Introduces MoisesDB, a publicly available multitrack dataset with 240 songs organized under a hierarchical stem taxonomy enabling fine-grained music source separation beyond the standard four-stem setting, and provides baseline benchmarking results with open-source and oracle methods;Lack of publicly available datasets supporting granular multi-stem music source separation beyond the common four-stem taxonomy (vocals, drums, bass, other);"Music source separation (4, 5, and 6 stems; extensible hierarchical stems)";Stereo multitrack recordings (professionally recorded);MoisesDB (240 songs, 47 artists, ~14h 24m total duration);"MoisesDB benchmark subsets (4 stems N=235; 5 stems N=104; 6 stems N=88)";"Real multitrack recordings (unmastered; minor bleeding possible)";-;"Additive stem mixing; hierarchical taxonomy grouping; no compression/EQ/mastering; loudness and dynamic range analysis; Python API for stem construction";Waveform (time-domain multitrack and stem mixtures);-;-;"HT-Demucs; Spleeter; IBM; IRM; MWF (benchmark models only)";-;"SDR (Source-to-Distortion Ratio; mean, std, median)";"HT-Demucs achieves highest overall SDR among evaluated methods (e.g., 4-stem overall mean 9.91 dB; 6-stem overall mean 6.24 dB); oracle IRM/MWF competitive; performance decreases as stem granularity increases"
10;Défossez, A. (2021). Hybrid spectrogram and waveform source separation. arXiv preprint arXiv:2111.03600.;Bu çal??ma, müzik kaynak ay?rma modellerinin zaman alan? (waveform) ve frekans alan? (spektrogram) yakla??mlar?n? tek bir çat? alt?nda birle?tiren hibrit ve uçtan uca (end-to-end) bir çözüm sunmaktad?r. Önerilen hibrit Demucs mimarisi, her kaynak için hangi alan?n daha uygun oldu?una modelin kendisinin karar vermesine ve gerekirse iki alan?n birlikte kullan?lmas?na olanak tan?maktad?r. Mimari, Music Demixing Challenge 2021 yar??mas?n? kazanm?? ve s?k??t?r?lm?? residual dallar, yerel attention mekanizmalar? ve tekil de?er düzenlile?tirme gibi ek iyile?tirmelerle güçlendirilmi?tir. Deneysel sonuçlar, MusDB HQ veri kümesi üzerinde ortalama 1.4 dB SDR art??? sa?land???n? ve bu kazan?m?n insan dinleme testleriyle de do?ruland???n? göstermektedir. Bu yönleriyle çal??ma, hibrit alan modellemenin MSS için güçlü ve pratik bir yakla??m oldu?unu ortaya koymaktad?r.;Hybrid Spectrogram and Waveform Source Separation;2021;Workshop (MDX Workshop);Proposes Hybrid Demucs, a dual-branch U-Net combining waveform and spectrogram representations with shared core layers, compressed residual branches, local attention, and stabilization techniques, achieving state-of-the-art performance and winning the MDX 2021 challenge (Track A);Limitations of purely waveform or purely spectrogram-based music source separation models and need for improved representation and artifact reduction;Music source separation (4 stems: drums, bass, vocals, other);Stereo (2-channel, 44.1 kHz);"MUSDB18-HQ (86 train, 14 valid; 150 tracks total); additional 150 internal tracks for Track B; realistic remix fine-tuning dataset";"MUSDB18-HQ test set (50 tracks); MDX hidden test set (27 tracks)";Real multitrack recordings + synthetic realistic remixes for fine-tuning;"Random shifts; realistic remixing with beat alignment, tempo adjustment (<15%), pitch shift (<3 semitones); longer segment training (30s fine-tuning)";"STFT (4096 window, hop 1024); complex-as-channels or amplitude spectrogram; waveform up/downsampling; aligned padding; ISTFT reconstruction";Waveform + Time-Frequency (Hybrid);Supervised;Hybrid CNN U-Net with BiLSTM and Local Attention;"Hybrid Demucs; Demucs v2 (baseline); bag of 4 models for competition";"Waveform-domain L1 loss (applied after ISTFT + waveform summation); SVD regularization; EMA stabilization";"SDR (Vincent et al.); nSDR (MDX definition); MOS (quality and bleeding)";"MDX Track A: 7.33 nSDR overall (1st place); Track B: 8.11 nSDR overall; MUSDB-HQ: 7.68 dB SDR overall (?+1.4 dB over original Demucs); strong improvements on drums and bass; improved MOS and reduced bleeding"
11;Mitsufuji, Y., Fabbro, G., Uhlich, S., Stöter, F. R., Défossez, A., Kim, M., ... & Cheuk, K. W. (2022). Music demixing challenge 2021. Frontiers in Signal Processing, 1, 808395.;"Bu çal??ma, müzik kaynak ay?rma alan?nda kullan?lan de?erlendirme kampanyalar? ve veri kümelerinin s?n?rl?l?klar?n? ele alarak Music Demixing Challenge’? tan?tmaktad?r. Özellikle MUSDB18 veri kümesinin Bat? pop müzi?ine odakl? yap?s? ve s?n?rl? say?da miks mühendisiyle üretilmi? olmas? nedeniyle ortaya ç?kan önyarg?lara dikkat çekilmektedir. Önerilen yar??ma, farkl? disiplinlerden ara?t?rmac?lar?n kat?l?m?n? kolayla?t?ran bir yap? sunmakta; de?erlendirmeyi yaln?zca organizatörlerin eri?ebildi?i, profesyonelce haz?rlanm?? gizli bir test seti üzerinden gerçekle?tirmektedir. Ayr?ca daha geni? müzik türleri ve daha fazla miks mühendisi içeren bir veri da??l?m? hedeflenmektedir. Çal??ma, bu yar??man?n veri setleri, temel modelleri, de?erlendirme metrikleri ve sonuçlar?n? ayr?nt?l? biçimde sunarak, gelecekteki müzik kaynak ay?rma yar??malar? için metodolojik bir çerçeve sa?lamaktad?r.";Spleeter: A Fast and Efficient Music Source Separation Tool with Pre-trained Models;2019;Conference (ISMIR Late-Breaking/Demo);Introduces Spleeter, an open-source deep learning-based music source separation tool providing pretrained models for 2-, 4-, and 5-stem separation with fast inference suitable for production use;Need for fast, accessible, and high-quality music source separation models for practical and industrial applications;Music source separation (2, 4, and 5 stems: vocals, accompaniment, drums, bass, piano);Stereo (2-channel);Deezer internal multitrack dataset (NG size details);MUSDB18 test set;Real multitrack recordings + internally curated proprietary dataset;"Random mixing of stems; segment cropping (NG detailed parameters)";"STFT magnitude spectrogram input; U-Net architecture predicting soft masks; Wiener filtering post-processing; ISTFT reconstruction";Time-Frequency (magnitude spectrogram);Supervised;CNN-based U-Net;Spleeter (2-stem, 4-stem, 5-stem models);L2 loss on magnitude spectrogram (mask estimation objective);SDR (museval / BSS Eval);"Competitive SDR on MUSDB18 (e.g., ~5.9 dB overall for 4-stem model); significantly faster inference than prior methods"
12;Kong, Q., Cao, Y., Liu, H., Choi, K., & Wang, Y. (2021). Decoupling magnitude and phase estimation with deep resunet for music source separation. arXiv preprint arXiv:2109.05418.;Bu çal??ma, geleneksel sadece genlik (magnitude) temelli müzik kaynak ay?rma yakla??mlar?n?n s?n?rlamalar?n? ele alarak faz bilgisini de do?rudan modelleyen yeni bir yöntem önermektedir. Yazarlar, ay?rma sürecinde complex Ideal Ratio Mask (cIRM) tahminine odaklanarak genlik ve faz bile?enlerini ayr??t?r?lm?? biçimde ö?renmekte ve böylece faz yeniden olu?turma hatalar?n? azaltmaktad?r. Ayr?ca maske de?erlerinin 1 ile s?n?rland?r?lmas?n?n gerçekçi olmad???n? göstererek, 1’in üzerinde genlik de?erlerine izin veren bir ay?rma stratejisi sunmaktad?r. Mimari aç?dan ise çok derin a?lar?n potansiyelini ara?t?rmak amac?yla 143 katmana kadar uzanan residual UNet tasar?m? önerilmektedir. MUSDB18 veri kümesi üzerinde elde edilen sonuçlar, özellikle vokal ay?rmada 8.98 dB SDR ile önceki en iyi sonuçlar? belirgin ?ekilde a?arak yöntemin MSS literatürüne güçlü bir katk? sundu?unu göstermektedir.;Decoupling Magnitude and Phase Estimation with Deep ResUNet for Music Source Separation;2021;Conference (ISMIR);Proposes decoupled magnitude and phase estimation for complex ideal ratio masks (cIRM), introduces combination of bounded mask and direct magnitude prediction, and develops a very deep 143-layer Residual U-Net to improve music source separation performance;"Limitations of magnitude-only mask estimation, bounded mask values, and shallow architectures in music source separation; need for accurate phase modeling and unbounded mask estimation";Music source separation (vocals, accompaniment, bass, drums, other);Stereo (2-channel, 44.1 kHz);"MUSDB18 (86 train, 14 validation from training split; 100 train / 50 test official split)";MUSDB18 test set;Real multitrack recordings;"Mix-audio data augmentation (randomly mixing two 3-second segments from same source except bass); segment cropping (3s)";"STFT (Hann window 2048, hop 441); magnitude and complex mask modeling; waveform reconstruction via iSTFT";Time-Frequency (complex spectrogram with cIRM);Supervised;Deep CNN (Residual U-Net, 143 layers);"ResUNetDecouple+ (143-layer); UNetDecouple+ (33-layer baseline variants)";Waveform-domain L1 loss;"SDR (museval; Vincent et al.)";"State-of-the-art on MUSDB18 at publication; 8.98 dB SDR on vocals; improved performance across bass, drums, other, and accompaniment compared to prior methods"
13;Subakan, C., Ravanelli, M., Cornell, S., Bronzi, M., & Zhong, J. (2021, June). Attention is all you need in speech separation. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 21-25). IEEE.;Bu çal??ma, s?ral? hesaplama gereksinimi nedeniyle ölçeklenebilirli?i s?n?rl? olan RNN tabanl? yakla??mlara alternatif olarak, tamamen Transformer tabanl? ve RNN’siz bir konu?ma ay?rma modeli olan SepFormer’? tan?tmaktad?r. SepFormer, çok ölçekli Transformer yap?s? sayesinde k?sa ve uzun dönemli ba??ml?l?klar? birlikte modelleyebilmekte ve böylece konu?ma ay?rma performans?n? art?rmaktad?r. Model, WSJ0-2mix ve WSJ0-3mix veri kümeleri üzerinde state-of-the-art sonuçlara ula??rken, Transformer’lar?n paralelle?tirilebilir yap?s?n? etkin biçimde kullanarak yüksek h?z ve bellek verimlili?i sa?lamaktad?r. Özellikle kodlanm?? temsillerin güçlü biçimde a?a?? örneklenmesine ra?men rekabetçi performans sunmas?, SepFormer’?n verimli ve ölçeklenebilir bir ay?rma mimarisi oldu?unu göstermektedir.;Attention Is All You Need in Speech Separation;2021;Conference (ICASSP);Proposes SepFormer, an RNN-free Transformer-based speech separation architecture using a dual-path multi-scale Transformer masking network to model short- and long-term dependencies, achieving state-of-the-art SI-SNRi on WSJ0-2mix and WSJ0-3mix while improving speed and memory efficiency;Limitations of RNN-based sequence modeling (sequential processing, limited parallelization, high memory usage) in monaural speech separation;Monaural speech separation (2-speaker and 3-speaker mixtures);Single-channel (mono);"WSJ0-2mix (30h train, 10h validation, 5h test); WSJ0-3mix (same protocol)";"WSJ0-2mix test set; WSJ0-3mix test set";Synthetic mixtures created by randomly mixing clean WSJ0 utterances (0–5 dB relative levels);"Dynamic mixing (on-the-fly mixture generation); speed perturbation (95%–105%)";"Time-domain learned encoder (Conv1D, kernel 16, stride 8); chunking with 50% overlap; dual-path processing; layer normalization; positional encoding; overlap-add reconstruction";Waveform (time-domain learned representation);Supervised (Permutation Invariant Training);Transformer (Dual-Path Transformer architecture);SepFormer;Scale-Invariant Signal-to-Noise Ratio (SI-SNR) with utterance-level permutation invariant loss (clipped at 30 dB);"SI-SNRi; SDRi";"WSJ0-2mix: 22.3 dB SI-SNRi and 22.4 dB SDRi (with dynamic mixing); WSJ0-3mix: 19.5 dB SI-SNRi and 19.7 dB SDRi; state-of-the-art at publication time"
14;Hennequin, R., Khlif, A., Voituret, F., & Moussallam, M. (2020). Spleeter: a fast and efficient music source separation tool with pre-trained models. Journal of Open Source Software, 5(50), 2154.;Bu çal??ma, müzik kaynak ay?rma için Spleeter adl?, önceden e?itilmi? modellere sahip, h?zl? ve kullan?c? dostu bir araç sunmaktad?r. TensorFlow tabanl? olan Spleeter, tek bir komutla müzik dosyalar?n? 2, 4 veya 5 stem (ör. vokal, davul, bas, piyano) halinde ay?rabilmekte ve ayn? zamanda mevcut modellerin yeniden e?itilmesine veya ince ayar yap?lmas?na olanak tan?maktad?r. Yay?nlanan önceden e?itilmi? modeller, MUSDB18 üzerinde state-of-the-art’e yak?n performans sergileyerek aç?k kaynakl? araçlar aras?nda öne ç?kmaktad?r. Ayr?ca sistem, tek bir GPU üzerinde gerçek zaman?n çok üzerinde h?zlarda çal??abilmesiyle, ara?t?rma ve pratik uygulamalar için MSS’nin eri?ilebilirli?ini önemli ölçüde art?rmaktad?r.;Spleeter: a fast and efficient music source separation tool with pre-trained models;2020;Journal (JOSS);Releases Spleeter, a fast TensorFlow-based music source separation tool with publicly available pretrained U-Net models for 2-, 4-, and 5-stem separation, achieving near state-of-the-art performance while enabling very high inference speed;Providing an accessible, fast, and high-performing music source separation system with pretrained models for research and industrial use;Music source separation (2, 4, and 5 stems: vocals, drums, bass, piano, other);Stereo (2-channel);"Deezer internal multitrack datasets (including Bean dataset; not publicly released)";MUSDB18 test set;"Real multitrack recordings (training on proprietary real stems; evaluation on MUSDB18)";NG;"STFT magnitude spectrogram input; U-Net encoder-decoder CNN (12 layers); soft masking; optional multi-channel Wiener filtering (Norbert)";Time-Frequency (magnitude spectrogram);Supervised;CNN (U-Net);Spleeter (2-stem, 4-stem, 5-stem pretrained models);L1 loss between masked mixture spectrogram and target source spectrogram;"SDR; SIR; SAR; ISR";"MUSDB18 (4-stem, MWF): Vocals 6.86 dB SDR; Bass 5.51 dB; Drums 6.71 dB; Other 4.55 dB; competitive with Open-Unmix and close to Demucs; ~100× faster than real-time on GPU"
15;Slizovskaia, O., Haro, G., & Gómez, E. (2021). Conditioned source separation for musical instrument performances. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 29, 2083-2095.;Bu çal??ma, ayn? anda çalan birden fazla ve timbral olarak benzer müzik enstrüman?n?n kaynak ay?rma sürecinde yaratt??? zorluklar? ele almaktad?r. Yöntem, yaln?zca ses sinyaline dayal? ay?rma yerine, modele ek ba?lamsal bilgilerle ko?ulland?rma (conditioning) yakla??m?n? önermektedir. Bu kapsamda, enstrümanlar?n kar???mda bulunup bulunmad???na dair bilgiler ve ilgili video ak??? gibi iki ek veri kipli?i kullan?larak ay?rma a??n?n farkl? katmanlar?nda entegrasyon stratejileri incelenmektedir. Sonuçlar, ses d??? bilgilerin uygun ?ekilde modele dahil edilmesinin, özellikle yüksek korelasyona sahip enstrümanlar?n ayr?m?nda ay?rma kalitesini anlaml? biçimde art?rabildi?ini göstermektedir.;Demucs: Deep Extractor for Music Sources with Extra Unlabeled Data Remixed;2019;Conference (ISMIR);Introduces Demucs, a waveform-domain encoder-decoder convolutional architecture with skip connections for end-to-end music source separation, and proposes leveraging additional unlabeled data through realistic remixing to improve performance;Improving music source separation in the waveform domain without relying on spectrogram representations and enhancing performance using extra unlabeled data;Music source separation (4 stems: vocals, drums, bass, other);Stereo (2-channel);MUSDB18-HQ + 800 additional unlabeled songs for remix-based data augmentation;MUSDB18-HQ test set;Real multitrack recordings + synthetic realistic remixes from extra unlabeled data;"Realistic remixing of stems from additional songs; random segment sampling; data balancing strategies";"Raw waveform input; strided convolutional encoder-decoder; bidirectional LSTM in bottleneck; overlap-add at inference";Waveform (time-domain);Supervised;CNN encoder-decoder with BiLSTM bottleneck;Demucs;L1 loss on waveform;SDR (museval / BSS Eval);"7.05 dB SDR on vocals (MUSDB18); competitive with spectrogram-based models; improved performance with extra remix data"
16;Wisdom, S., Tzinis, E., Erdogan, H., Weiss, R., Wilson, K., & Hershey, J. (2020). Unsupervised sound separation using mixture invariant training. Advances in neural information processing systems, 33, 3846-3857.;"Bu çal??ma, sentetik ve etiketli e?itim verilerine ba??ml? olan denetimli kaynak ay?rma yakla??mlar?n?n s?n?rlamalar?n? ele alarak MixIT (Mixture Invariant Training) adl? tamamen denetimsiz bir ö?renme yöntemini önermektedir. MixIT, yaln?zca tek kanall? gerçek kar???mlar? kullanarak e?itilir; e?itim s?ras?nda mevcut kar???mlar yeniden kar??t?r?l?r ve model bu sinyalleri, yeniden birle?tirildi?inde orijinal kar???mlar? yakla??k olarak üreten gizli kaynaklara ay?rmay? ö?renir. Bu yakla??m, gerçek dünya seslerinin akustik çe?itlili?ine daha iyi uyum sa?lamay? mümkün k?lmaktad?r. Çal??ma, MixIT’in konu?ma ay?rmada denetimli yöntemlerle rekabetçi performans sundu?unu ve yar? denetimli kullan?mda alan uyarlamas?, yank?l? konu?ma ay?rma, gürültü giderme ve gerçek ortam verilerinden ö?renme gibi senaryolarda önemli kazan?mlar sa?lad???n? göstermektedir.";Unsupervised Sound Separation Using Mixture Invariant Training;2020;Conference (NeurIPS);Proposes MixIT, a fully unsupervised training framework for single-channel sound separation using mixtures of mixtures and a mixture-invariant loss that generalizes permutation invariant training, enabling competitive performance without ground-truth source signals and strong semi-supervised domain adaptation;Supervised separation relies on synthetic mixtures with clean ground-truth sources, causing domain mismatch and limiting use of real-world unlabeled mixtures;"Single-channel speech separation; speech enhancement; universal sound separation";Single-channel (mono);"WSJ0-2mix; Libri2Mix; reverberant WSJ0-2mix; reverberant Libri2Mix; FUSS (16h); LibriSpeech (for enhancement); YFCC100m (1600h unlabeled); Freesound-derived noise data";"WSJ0-2mix test set; Libri2Mix test set; FUSS test set; YFCC100m evaluation subsets";"Synthetic mixtures (WSJ0-2mix; Libri2Mix; FUSS) + real in-the-wild mixtures (YFCC100m; Freesound); mixture-of-mixtures constructed during training";"Dynamic mixture-of-mixtures construction; random pairing without replacement; supervised/unsupervised mixing ratio sweep; optional zeroing of supervised mixtures (p0)";"Time-domain learned convolutional encoder; TDCN++ (Conv-TasNet style); mask estimation; overlap-add reconstruction; mixture consistency projection";Waveform (time-domain);Unsupervised + Semi-supervised + Supervised (PIT baseline);Time-domain convolutional network (Conv-TasNet / TDCN++ family);"MixIT with TDCN++; PIT baseline with same architecture";"Negative thresholded SNR loss (signal-level); SI-SNR-based objective; permutation/mixture invariant optimization over assignments";"SI-SNR; SI-SNR improvement (SI-SNRi); multi-source SI-SNRi (MSi); single-source SI-SNR (SS); MoMi (mixture-of-mixtures reconstruction metric)";"On WSJ0-2mix, fully unsupervised MixIT achieves performance comparable to supervised PIT (within ~0–3 dB depending on setup); strong gains (2–6 dB SI-SNRi) in domain adaptation with small amounts of matched unsupervised data; speech enhancement: 11.4 dB SI-SNRi (unsupervised) vs 15.0 dB (supervised); FUSS MSi up to 13.3 dB in semi-supervised setting"
17;Hung, Y. N., & Lerch, A. (2020). Multitask learning for instrument activation aware music source separation. arXiv preprint arXiv:2008.00616.;Bu çal??ma, müzik kaynak ay?rma sistemlerinin genellikle yaln?zca ay?rma görevine odaklanmas?n? sorgulayarak, çok görevli (multitask) bir ö?renme yakla??m? önermektedir. Önerilen yap?, kaynak ay?rmay? enstrüman aktivasyon tespiti ile birlikte ele alarak, bu ek bilginin ay?rma kalitesini art?r?p art?ramayaca??n? ara?t?rmaktad?r. Ayr?ca çal??ma, MUSDB’deki s?n?rl? enstrüman say?s?n?n ötesine geçerek alt? ba??ms?z enstrüman içeren daha gerçekçi bir senaryoda de?erlendirme yapmaktad?r. MedleyDB ve Mixing Secrets veri kümelerinin birlikte kullan?ld??? deneyler, multitask modelin Open-Unmix taban çizgisine k?yasla daha iyi performans sa?lad???n? ve ayn? zamanda MUSDB üzerinde rekabetçi sonuçlar? korudu?unu göstermektedir.;Multitask Learning for Instrument Activation Aware Music Source Separation;2020;Conference (ISMIR);Proposes a U-Net-based multitask model (IASS) that jointly learns instrument activation detection and music source separation, and uses predicted instrument activity as a time-frame weight during inference to suppress non-target frames and improve separation quality;"Most source separation systems ignore related MIR tasks; limited instrument categories in MUSDB; need to leverage instrument activation to improve separation and support more instruments";Music source separation (up to 6 instruments: Vocals, Bass, Drums, Electric Guitar, Acoustic Guitar, Piano);"Single-channel (mono; downmixed from stereo for MM dataset; stereo for MUSDB-HQ but processed as magnitude spectrogram)";"MUSDB-HQ (86 train / 14 val / 50 test); MM dataset (MedleyDB + Mixing Secrets; 488 train / 100 test songs)";"MUSDB-HQ test split; MM test split (filtered per instrument presence)";Real multitrack recordings remixed with data augmentation;"Random gain (0.25–1.25); random 6s chunking; random remixing of target + 1–5 accompaniment tracks; loudness normalization (MM)";"STFT magnitude spectrogram (window 4096; hop 1024; 44.1 kHz); U-Net with residual blocks; instrument classifier on latent vector; median filtering of predicted activity; phase reuse for reconstruction";Time-Frequency (magnitude spectrogram);Supervised (multitask learning: separation + instrument activation);CNN U-Net with residual blocks (multitask architecture);"IASS (Instrument Aware Source Separation); Open-Unmix baseline";"MSE loss (source separation) + Binary Cross-Entropy loss (instrument activation); weighted sum L = LMSE + ?LBCE";"SDR; SIR; SAR; ISR (BSS Eval via museval); AUC for instrument activation";"On MUSDB-HQ: IASS improves SDR for Vocals (6.46 vs 6.11) and Drums (5.56 vs 5.02) over Open-Unmix; comparable on Other; slightly lower on Bass; on MM dataset: IASS outperforms Open-Unmix across 6 instruments (e.g., Vocals 4.78 vs 3.68 SDR; Bass 5.26 vs 4.04); median filtering and oracle activity further improve average SDR/SIR/SAR"
18;Manilow, E., Seetharaman, P., & Pardo, B. (2020, May). Simultaneous separation and transcription of mixtures with multiple polyphonic and percussive instruments. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 771-775). IEEE.;Bu çal??ma, müzik kaynak ay?rma ve müzik transkripsiyonunu tek bir derin ö?renme mimarisi alt?nda birlikte ele alan Cerberus adl? çok görevli bir yakla??m sunmaktad?r. Cerberus, Chimera a??n? temel alarak buna üçüncü bir transkripsiyon ba?l??? eklemekte ve her görevi farkl? kay?p fonksiyonlar?yla e?iterek ortak bir müzikal temsil ö?renmektedir. Bu sayede model, tek bir a? ile be? enstrümana kadar hem ay?rma hem de insan taraf?ndan okunabilir transkripsiyon üretme yetene?i kazanmaktad?r. Deneyler, ay?rma ve transkripsiyon görevlerinin birbirini tamamlay?c? oldu?unu ve birlikte ö?renildiklerinde her iki görevde de daha iyi performans ve daha güçlü genelleme sa?lad???n? göstermektedir.;Simultaneous Separation and Transcription of Mixtures with Multiple Polyphonic and Percussive Instruments;2020;Conference (ICASSP);Proposes Cerberus, a three-headed deep learning architecture that jointly performs source separation and automatic music transcription by extending the Chimera network with an additional transcription head, demonstrating improved performance and generalization through multitask learning;Learning to jointly separate and transcribe multiple simultaneous polyphonic and percussive instruments from musical mixtures using a shared representation;Music source separation + multi-instrument automatic music transcription (piano-roll per instrument);Single-channel (magnitude spectrogram input);"Slakh2100 (synthetic mixtures with aligned MIDI); additional evaluation mixtures built from MAPS and GuitarSet";"Slakh2100 test splits; constructed MAPS + GuitarSet mixtures (real recordings mixed incoherently)";Synthetic (Slakh2100) + real recordings (MAPS, GuitarSet) for evaluation;"Segment selection with ?10 note onsets; random subset instrument mixing; 5-second segment extraction";"STFT magnitude spectrogram (1024 window, 256 hop, 16 kHz); 4-layer BLSTM stack (300 hidden units each); three output heads (deep clustering, mask inference, transcription); sigmoid/softmax activations; thresholding for piano roll output";Time-Frequency (magnitude spectrogram);Supervised (multitask learning);RNN-based (BLSTM) with multitask heads (Chimera + Transcription);Cerberus (Deep Clustering head + Mask Inference head + Transcription head);Weighted sum of losses: L = ?LDC + ?LMI + ?LTR (Deep Clustering loss + Mask Inference loss + L2 transcription loss);"SDR (scale-dependent); Precision; Recall; F1 (note on/off or onset for drums)";"On Slakh2100 (piano+guitar): up to 10.0 dB SDR with multitask setup; improved transcription F1 (~0.47) compared to single-task models; better generalization on real MAPS+GuitarSet mixtures (e.g., 5.0 dB SDR vs ?4.5 dB for single-task baselines)"
19;Stöter, F. R., Uhlich, S., Liutkus, A., & Mitsufuji, Y. (2019). Open-unmix-a reference implementation for music source separation. Journal of Open Source Software, 4(41), 1667.;"Bu çal??ma, müzik kaynak ay?rma alan?ndaki aç?k kaynak eksikli?ini gidermeyi amaçlayan Open-Unmix adl? bir referans sistemi tan?tmaktad?r. Open-Unmix, derin ö?renme tabanl? state-of-the-art ay?rma performans?n? aç?k kaynak olarak sunarak hem akademik ara?t?rmalar hem de pratik kullan?m için önemli bir bo?lu?u doldurmaktad?r. Sistem, farkl? derin ö?renme çat?lar? için esnek uygulamalar sa?layarak tekrarlanabilir ara?t?rmay? kolayla?t?rmakta; ayn? zamanda önceden e?itilmi? modeller ile son kullan?c?lar?n ve sanatç?lar?n do?rudan kaynak ay?rma yapabilmesine olanak tan?maktad?r. Çal??ma, Open-Unmix’i veri kümeleri, yaz?l?m araçlar? ve aç?k de?erlendirme bile?enleriyle birlikte aç?k ve sürdürülebilir bir müzik kaynak ay?rma ekosisteminin temel yap? ta?? olarak konumland?rmaktad?r.";Open-Unmix - A Reference Implementation for Music Source Separation;2019;Journal (JOSS);Provides an open-source reference implementation of a state-of-the-art deep learning system for music source separation, including pretrained models, reproducible training pipeline, and integration within the sigsep ecosystem to enable standardized benchmarking and research reproducibility;Lack of an open-source state-of-the-art baseline for music source separation, leading to inconsistent comparisons and limited reproducibility in research;Music source separation (4 stems: vocals, drums, bass, other);Stereo (2-channel);"MUSDB18; MUSDB18-HQ";MUSDB18 test set (SiSEC evaluation framework);Real multitrack recordings (MUSDB18);NG;"STFT magnitude spectrogram input; normalization; bidirectional LSTM core; mask estimation; inverse STFT reconstruction; optional multi-channel Wiener filtering (Norbert)";Time-Frequency (magnitude spectrogram);Supervised;RNN-based (BiLSTM);Open-Unmix (UMX);MSE loss between predicted and target magnitude spectrograms (mask-based objective);SDR (SiSEC / museval evaluation);"Reaches state-of-the-art open-source performance on MUSDB18; statistically comparable to best SiSEC system (TAK1); competitive SDR across vocals, drums, bass, and other stems"
20;Meseguer-Brocal, G., & Peeters, G. (2019). Conditioned-U-Net: Introducing a control mechanism in the U-Net for multiple source separations. arXiv preprint arXiv:1907.01277.;Bu çal??ma, her enstrüman için ayr? ayr? e?itilen kaynak ay?rma modellerinin verimsizli?ine çözüm olarak Conditioned-U-Net (C-U-Net) adl? ko?ulland?rmal? bir mimari önermektedir. C-U-Net, standart U-Net yap?s?na kontrol edilebilir bir mekanizma ekleyerek tek bir modelin farkl? enstrümanlar? ay?rabilmesini sa?lamaktad?r. Ay?r?lacak enstrüman, one-hot kodlu bir ko?ulland?rma vektörü ile modele iletilmekte ve bu bilgi FiLM (Feature-wise Linear Modulation) katmanlar? arac?l???yla a??n ara temsillerini yönlendirmektedir. Bu sayede model, farkl? görevler için ayr? a?lar e?itmeye gerek kalmadan, tek bir a? ile özel modellere e?de?er performans sunarken hesaplama ve model maliyetini dü?ürmektedir.;;;;;;;;;;;;;;;;;;;
21;Défossez, A., Usunier, N., Bottou, L., & Bach, F. (2019). Music source separation in the waveform domain. arXiv preprint arXiv:1911.13254.;"Bu çal??ma, müzik kaynak ay?rmada dalga formu (waveform) alan?nda çal??an yakla??mlar? ele alarak, spektrogram-temelli maskelere dayanan yayg?n yöntemlere güçlü bir alternatif sunmaktad?r. Yazarlar, konu?ma ay?rma için geli?tirilen Conv-TasNet’i müzik ay?rmaya uyarlamakta; bu modelin birçok spektrogram yöntemini geçmesine ra?men i?itsel artefaktlar üretti?ini göstermektedir. Buna kar??l?k, U-Net yap?s? ve çift yönlü LSTM içeren yeni bir waveform-to-waveform mimari olan Demucs önerilmektedir. MusDB veri kümesi üzerindeki deneyler, uygun veri art?rma ile Demucs’un mevcut state-of-the-art yöntemleri geride b?rakt???n? ve özellikle do?al ses kalitesi aç?s?ndan insan de?erlendirmelerinde belirgin üstünlük sa?lad???n? ortaya koymaktad?r. Ayr?ca modelin do?ruluk kayb? olmadan s?k??t?r?labilmesi, Demucs’u pratik ve yüksek kaliteli bir çözüm haline getirmektedir; ancak baz? kaynaklar aras?nda (özellikle vokal–di?er) s?n?rl? s?z?nt?lar gözlemlenmektedir.";;;;;;;;;;;;;;;;;;;
22;Kavalerov, I., Wisdom, S., Erdogan, H., Patton, B., Wilson, K., Le Roux, J., & Hershey, J. R. (2019, October). Universal sound separation. In 2019 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA) (pp. 175-179). IEEE.;"Bu çal??ma, konu?ma odakl? geli?tirilmi? derin ö?renme tabanl? ay?rma yöntemlerinin rastgele ve farkl? türde seslerden olu?an kar???mlar için ne ölçüde genellenebilir oldu?unu inceleyerek evrensel ses ay?rma (universal sound separation) problemine odaklanmaktad?r. Bu amaçla, konu?ma ve konu?ma-d??? sesleri içeren yeni bir veri kümesi olu?turulmu? ve farkl? maske-tabanl? mimariler sistematik olarak kar??la?t?r?lm??t?r. Çal??ma, ConvTasNet’ten esinlenen zaman-alan? mimariler ile STFT tabanl? yakla??mlar?; farkl? pencere uzunluklar? ve ö?renilebilir/klasik dönü?üm temsilleri aç?s?ndan analiz etmektedir. Sonuçlar, görev türüne ba?l? olarak farkl? zaman çözünürlüklerinin daha uygun oldu?unu ve evrensel ay?rma senaryosunda STFT tabanl? temsillerin ö?renilebilir temsillerden daha iyi performans gösterdi?ini ortaya koymaktad?r. Önerilen en iyi yöntemler, hem konu?ma/konu?ma-d??? ay?rmada hem de genel ses ay?rmada yüksek SDR kazan?mlar? sa?layarak, evrensel ay?rma için mimari ve temsil seçiminin kritik önemini vurgulamaktad?r.";;;;;;;;;;;;;;;;;;;
23;Manilow, E., Wichern, G., Seetharaman, P., & Le Roux, J. (2019, October). Cutting music source separation some Slakh: A dataset to study the impact of training data quality and quantity. In 2019 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA) (pp. 45-49). IEEE.;Bu çal??ma, müzik kaynak ay?rma ara?t?rmalar?nda kar??la??lan etiketli veri eksikli?ini ele alarak Slakh (Synthesized Lakh) adl? sentetik bir veri kümesini tan?tmaktad?r. Slakh, Lakh MIDI veri kümesindeki parçalar?n profesyonel sanal enstrümanlarla yeniden sentezlenmesi yoluyla olu?turulmu?, yüksek kaliteli kar???mlar ve kar??l?k gelen stem’ler sunmaktad?r. ?lk sürüm olan Slakh2100, MUSDB18’e k?yasla çok daha büyük ölçekli bir veri hacmi sa?layarak veri art?rma ve kar??la?t?rmal? de?erlendirme aç?s?ndan önemli bir avantaj sunmaktad?r. Çal??ma, Slakh’?n mevcut veri kümelerini etkili biçimde tamamlayabildi?ini ve veri yo?un müzik sinyali i?leme görevleri için ölçeklenebilir bir ara?t?rma altyap?s? olu?turdu?unu göstermektedir.;;;;;;;;;;;;;;;;;;;
24;Luo, Y., & Mesgarani, N. (2019). Conv-tasnet: Surpassing ideal time–frequency magnitude masking for speech separation. IEEE/ACM transactions on audio, speech, and language processing, 27(8), 1256-1266.;Bu çal??ma, konu?ma ay?rmada yayg?n olarak kullan?lan zaman–frekans temsillerinin s?n?rlamalar?n? a?mak amac?yla tamamen zaman alan?nda çal??an uçtan uca bir model olan Conv-TasNet’i önermektedir. Conv-TasNet, dalga formunu ö?renilebilir bir do?rusal kodlay?c? ile ay?rmaya elveri?li bir temsile dönü?türmekte, bu temsile uygulanan maskeleme i?lemleriyle konu?mac?lar? ay?rmakta ve ard?ndan do?rusal bir kod çözücü ile tekrar zaman alan?na geri dönmektedir. Modelin merkezinde yer alan seyreltilmi? (dilated) zamansal evri?im bloklar?, uzun dönemli ba??ml?l?klar? dü?ük gecikme ve küçük model boyutu ile ö?renebilmektedir. Deneyler, Conv-TasNet’in hem iki hem de üç konu?mac?l? kar???mlarda önceki zaman–frekans tabanl? yöntemleri belirgin biçimde geride b?rakt???n? ve dü?ük gecikmesi sayesinde gerçek zamanl? uygulamalara uygun güçlü bir çözüm sundu?unu göstermektedir.;;;;;;;;;;;;;;;;;;;
25;Jansson, A., Bittner, R. M., Ewert, S., & Weyde, T. (2019, September). Joint singing voice separation and f0 estimation with deep u-net architectures. In 2019 27th European Signal Processing Conference (EUSIPCO) (pp. 1-5). IEEE.;"Bu çal??ma, vokal kaynak ay?rma ile temel frekans (F0) kestirimi görevlerinin birbirini tamamlay?c? yap?s?n? ele alarak bu iki görevi birlikte ö?renen (joint learning) yakla??mlar önermektedir. Farkl? mimari yap?lar?n kar??la?t?r?ld??? çal??mada, önce vokal ay?rma yap?p ard?ndan frekans kestirimi gerçekle?tiren ard???k (stacked) mimarinin en etkili çözüm oldu?u gösterilmektedir. Ortak ö?renme sayesinde hem ay?rma hem de F0 kestirimi performans? artmakta; özellikle vokal-F0 tahmininde iKala veri kümesi üzerinde state-of-the-art sonuçlara ula??lmaktad?r. Ayr?ca çal??ma, gerçek dünya müzikleri için polifonik vokal-F0 kestiriminin önemini vurgulayarak, mevcut tek sesli yakla??mlar?n s?n?rlamalar?na dikkat çekmektedir.";;;;;;;;;;;;;;;;;;;
26;Stoller, D., Ewert, S., & Dixon, S. (2018). Wave-u-net: A multi-scale neural network for end-to-end audio source separation. arXiv preprint arXiv:1806.03185.;"Bu çal??ma, genlik spektrogramuna dayal? yöntemlerin faz bilgisini ihmal etmesi ve ön-i?leme ba??ml?l?klar?n? a?mak amac?yla tamamen zaman alan?nda (waveform) çal??an Wave-U-Net mimarisini önermektedir. Wave-U-Net, U-Net yap?s?n? tek boyutlu zamansal sinyale uyarlayarak farkl? zaman ölçeklerinde özellikleri yeniden örnekleme (resampling) yoluyla uzun menzilli zamansal ba??ml?l?klar? modellemektedir. Çal??mada ayr?ca kaynaklar?n toplam?n?n kar???ma e?it olmas?n? zorlayan bir ç?k?? katman?, geli?tirilmi? upsampling yöntemi ve artefaktlar? azaltmaya yönelik ba?lam-duyarl? tahmin stratejileri sunulmaktad?r. Deneyler, Wave-U-Net’in ?ark? sesi ay?rmada spektrogram tabanl? state-of-the-art U-Net ile kar??la?t?r?labilir performans sa?lad???n? göstermekte; ayr?ca SDR metri?inde ayk?r? de?er problemlerine dikkat çekilerek daha sa?lam de?erlendirme yakla??mlar? önerilmektedir.";;;;;;;;;;;;;;;;;;;
27;Takahashi, N., Goswami, N., & Mitsufuji, Y. (2018, September). Mmdenselstm: An efficient combination of convolutional and recurrent neural networks for audio source separation. In 2018 16th International workshop on acoustic signal enhancement (IWAENC) (pp. 106-110). IEEE.;Bu çal??ma, ses kaynak ay?rma alan?nda ba?ar?l? olan MMDenseNet mimarisini geli?tirerek, uzun vadeli zamansal ba??ml?l?klar? daha etkili modelleyebilen çok ölçekli LSTM entegrasyonuna sahip yeni bir yap? önermektedir. Önerilen mimari, DenseNet tabanl? evri?imsel katmanlar?, skip connection’lar ile desteklenen LSTM bloklar?yla birle?tirerek hem yerel akustik özellikleri hem de uzun süreli yap?lar? birlikte ö?renebilmektedir. Deneysel sonuçlar, bu yakla??m?n MMDenseNet, yaln?zca LSTM ve bu a?lar?n basit birle?imlerine k?yasla daha yüksek ay?rma performans?, daha az parametre ve daha dü?ük hesaplama maliyeti sundu?unu göstermektedir. Özellikle ?ark? sesi ay?rma görevinde, ideal ikili maskeleri dahi a?an sonuçlar elde edilmesi, önerilen mimarinin verimli ve güçlü bir MSS çözümü oldu?unu ortaya koymaktad?r.;;;;;;;;;;;;;;;;;;;
28;Rafii, Z., Liutkus, A., Stöter, F. R., Mimilakis, S. I., FitzGerald, D., & Pardo, B. (2018). An overview of lead and accompaniment separation in music. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 26(8), 1307-1335.;"Bu çal??ma, vokal (lead) ve e?lik (accompaniment) ayr?m? problemine odaklanan kapsaml? bir derleme (review) sunmaktad?r. Makale, literatürdeki yöntemleri model-tabanl? ve veri-tabanl? yakla??mlar olarak s?n?fland?rmakta; model-tabanl? yöntemleri ise lead, e?lik ya da her ikisini birlikte hedeflemelerine göre incelemektedir. Veri-tabanl? yakla??mlar kapsam?nda, özellikle e?itim verisi elde etmenin zorluklar? ve son y?llarda öne ç?kan derin ö?renme tabanl? yöntemler tart???lmaktad?r. Ayr?ca çal??ma, müzik kaynak ay?rma sistemlerinin de?erlendirilmesindeki metrik sorunlar?n? ele almakta ve bu alandaki en kapsaml? kar??la?t?rmal? de?erlendirme sonuçlar?n? sunmaktad?r. Geni? referans listesi ve mevcut uygulamalara yönelik yönlendirmeleriyle, lead–accompaniment ay?rma literatürü için temel bir ba?vuru kayna?? niteli?i ta??maktad?r.";;;;;;;;;;;;;;;;;;;
29;Cano, E., FitzGerald, D., Liutkus, A., Plumbley, M. D., & Stöter, F. R. (2018). Musical source separation: An introduction. IEEE Signal Processing Magazine, 36(1), 31-40.;"Bu çal??ma, müzik kaynak ay?rma (MSS) problemini kapsaml? biçimde ele alan giri? ve derleme niteli?inde bir makaledir. Günlük hayatta remix, karaoke, upmixing gibi uygulamalar?n neden ayr? stem’lere ihtiyaç duydu?unu aç?klayarak, yaln?zca nihai kar???m mevcutken kaynaklar? ay?rman?n zorluklar?n? tart??maktad?r. Makale; müzik sinyallerinin harmonik yap?, zamansal tekrarlar ve enstrümana özgü frekans desenleri gibi ay?rt edici özelliklerinden nas?l yararlan?labilece?ini aç?klamakta, MSS için kullan?lan ba?l?ca model s?n?flar?n?, de?erlendirme metriklerini ve mevcut s?n?rlamalar? özetlemektedir. Ayr?ca gelecekteki ara?t?rma yönlerine i?aret ederek, MSS alan?na yeni ba?layanlar için temel bir referans ve çerçeve sunmaktad?r.";;;;;;;;;;;;;;;;;;;
30;Vincent, E., Virtanen, T., & Gannot, S. (Eds.). (2018). Audio source separation and speech enhancement. John Wiley & Sons.;"Bu kitap, ses kaynak ay?rma ve konu?ma iyile?tirme alanlar?n? kapsayan kapsaml? ve akademik bir ba?vuru kayna??d?r. Çal??ma; klasik sinyal i?leme yöntemlerinden ba?layarak, istatistiksel modeller, zaman–frekans temsilleri ve modern derin ö?renme tabanl? yakla??mlara kadar alan?n temel ve ileri düzey tekniklerini sistematik biçimde ele almaktad?r. Hem tek kanall? hem çok kanall? senaryolar, hem de gürültü bast?rma, konu?ma ay?rma ve genel ses ayr??t?rma problemleri detayl? olarak incelenmektedir. Editörlü bir kitap olmas? sayesinde, farkl? uzmanlar?n katk?lar?yla teori, algoritmalar, de?erlendirme metrikleri ve uygulama alanlar? bir arada sunulmakta; bu yönüyle kitap, ara?t?rmac?lar ve lisansüstü ö?renciler için temel bir referans eser niteli?i ta??maktad?r.";;;;;;;;;;;;;;;;;;;
31;Uhlich, S., Porcu, M., Giron, F., Enenkl, M., Kemp, T., Takahashi, N., & Mitsufuji, Y. (2017, March). Improving music source separation based on deep neural networks through data augmentation and network blending. In 2017 IEEE international conference on acoustics, speech and signal processing (ICASSP) (pp. 261-265). IEEE.;"Bu çal??ma, müzik sinyallerinin bireysel enstrüman izlerine ayr?lmas?n? ele alarak iki farkl? derin ö?renme mimarisini—bir ileri beslemeli (feed-forward) ve bir tekrarlayan (LSTM tabanl?) a?—kar??la?t?rmal? olarak incelemektedir. Her iki yakla??m?n da SiSEC DSD100 veri kümesi üzerinde state-of-the-art performans sa?lad??? gösterilmektedir. Çal??ma ayr?ca, özellikle tekrarlayan a?larda veri art?rman?n a??r? uyumu (overfitting) önlemede kritik oldu?unu vurgulamaktad?r. Bunun ötesinde, iki a??n ç?kt?lar?n?n do?rusal olarak birle?tirilmesi ve ard?ndan çok kanall? Wiener filtresi ile son i?lem uygulanmas?na dayanan bir blending stratejisi önerilmekte; bu yakla??m?n DSD100 üzerinde o tarihe kadarki en iyi sonuçlar? verdi?i gösterilmektedir.";;;;;;;;;;;;;;;;;;;
32;Chandna, P., Miron, M., Janer, J., & Gómez, E. (2017, February). Monoaural audio source separation using deep convolutional neural networks. In International conference on latent variable analysis and signal separation (pp. 258-266). Cham: Springer International Publishing.;Bu çal??ma, dü?ük gecikmeli tek kanall? müzik kaynak ay?rma için CNN tabanl? bir çerçeve önermektedir. Yöntem, zaman–frekans düzleminde yumu?ak maske (soft mask) tahmini yaparak kaynaklar? ay?rmakta ve vokal, davul, bas ile parça baz?nda de?i?en di?er enstrümanlar? içeren kar???mlar üzerinde de?erlendirilmektedir. Önerilen mimari, klasik çok katmanl? alg?lay?c? (MLP) ile kar??la?t?r?ld???nda benzer ay?rma kalitesi sunarken, i?lem süresinde belirgin bir h?z avantaj? sa?lamaktad?r. Kaynak ay?rma de?erlendirme kampanyalar?nda elde edilen rekabetçi sonuçlar, yöntemin gerçek zamanl?ya yak?n uygulamalar için uygun bir çözüm oldu?unu göstermektedir.;;;;;;;;;;;;;;;;;;;
33;Jansson, A., Humphrey, E., Montecchio, N., Bittner, R., Kumar, A., & Weyde, T. (2017). Singing voice separation with deep u-net convolutional networks.;Bu çal??ma, müzikte vokal–e?lik ayr?m?n?, kar???k bir spektrogramdan kaynak spektrogramlar?na dönü?üm yapan bir görüntüden-görüntüye çeviri problemi olarak ele almaktad?r. Bu amaçla, ince ayr?nt?lar? ba?ar?l? biçimde yeniden üretebilmesiyle bilinen U-Net mimarisi, ilk kez müzik kaynak ay?rma problemine uyarlanm??t?r. Nicel de?erlendirmeler ve dinleyici testleri, önerilen U-Net tabanl? yakla??m?n yüksek kaliteli ayr?mlar üretti?ini ve state-of-the-art performans sa?lad???n? göstermektedir.;;;;;;;;;;;;;;;;;;;
34;Luo, Y., Chen, Z., Hershey, J. R., Le Roux, J., & Mesgarani, N. (2017, March). Deep clustering and conventional networks for music separation: Stronger together. In 2017 IEEE international conference on acoustics, speech and signal processing (ICASSP) (pp. 61-65). IEEE.;Bu çal??ma, deep clustering yakla??m?n?n müzik kaynak ay?rma ba?lam?ndaki etkinli?ini inceleyerek, özellikle ?ark? sesi (vokal) ay?rma görevinde geleneksel uçtan uca a?lara k?yasla üstünlüklerini ortaya koymaktad?r. Deep clustering, do?rudan kaynak sinyallerini tahmin etmek yerine, her zaman–frekans noktas?n? gömme (embedding) uzay?nda temsil edip kümeleyerek ay?rma yapmas?yla ayr??maktad?r. Deneyler, bu yöntemin hem uyumlu hem de uyumsuz ko?ullarda geleneksel a?lardan daha iyi performans sergiledi?ini göstermektedir. Ayr?ca çal??mada, deep clustering’in esnek hedef fonksiyonu ile klasik a?lar?n güçlü sinyal tahmin yeteneklerinin birbirini tamamlay?c? oldu?u gösterilerek, bu iki yakla??m? birle?tiren hibrit bir çok-görevli modelin her iki bile?enden de daha üstün sonuçlar verdi?i ortaya konmaktad?r.;;;;;;;;;;;;;;;;;;;
35;Rafii, Z., Liutkus, A., Stöter, F. R., Mimilakis, S. I., & Bittner, R. (2017). The MUSDB18 corpus for music separation.;Bu çal??ma, müzik kaynak ay?rma ara?t?rmalar? için geli?tirilmi? MUSDB18 veri kümesini tan?tmaktad?r. Veri kümesi, farkl? müzik türlerinden olu?an 150 tam uzunlukta stereo parça ile bunlara kar??l?k gelen orijinal kaynaklar? (stem’leri) içermekte ve e?itim/test olarak ayr?lm?? bir yap? sunmaktad?r. MUSDB18’in temel amac?, kaynak ay?rma algoritmalar?n?n tasar?m?, kar??la?t?r?lmas? ve nesnel olarak de?erlendirilmesi için standart bir referans sa?lamakt?r. Karaoke gibi uygulamalar? hedefleyen bu veri kümesi, SiSEC 2018 kapsam?nda profesyonel müzik kay?tlar? için resmi de?erlendirme seti olarak kullan?lm?? ve MSS literatüründe en yayg?n kullan?lan benchmark haline gelmi?tir.;;;;;;;;;;;;;;;;;;;
36;Liutkus, A., Stöter, F. R., Rafii, Z., Kitamura, D., Rivet, B., Ito, N., ... & Fontecave, J. (2017, February). The 2016 signal separation evaluation campaign. In International conference on latent variable analysis and signal separation (pp. 323-332). Cham: Springer International Publishing.;Bu çal??ma, SiSEC 2016 (Signal Separation Evaluation Campaign) kapsam?nda düzenlenen topluluk temelli de?erlendirme sonuçlar?n? özetlemektedir. Kampanya, üçü konu?ma ve müzik sinyallerinin ayr??t?r?lmas?na, biri ise biyomedikal sinyal ayr?m?na odaklanan toplam dört farkl? görev içermektedir. Makalede, her bir görev k?saca tan?t?lmakta ve kat?lan sistemlerin kar??la?t?rmal? performanslar? sunulmaktad?r. Ayr?ca elde edilen sonuçlar ?????nda, kaynak ay?rma alan?ndaki güncel e?ilimler ve gelecekteki ara?t?rma yönleri tart???larak SiSEC’in alan için oynad??? merkezi rol vurgulanmaktad?r.;;;;;;;;;;;;;;;;;;;
37;Seetharaman, P., Pishdadian, F., & Pardo, B. (2017, October). Music/voice separation using the 2d fourier transform. In 2017 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA) (pp. 36-40). IEEE.;Bu çal??ma, ?ark? sesi ay?rma problemine tamamen farkl? ve sezgisel bir bak?? aç?s? getirerek 2B Fourier Dönü?ümü (2DFT) tabanl? yeni bir yöntem önermektedir. Yakla??m, müzik ve vokal sinyallerindeki periyodik yap?lar?n 2DFT düzleminde belirgin desenler olu?turdu?u gözlemine dayanmakta ve bu desenleri kullanarak kaynaklar? ay?rmaktad?r. Yöntem, biyolojik i?itsel sistemler ve görüntü i?leme literatürüyle kavramsal ba?lant?lar kurmas?yla dikkat çekmektedir. Önerilen sistemin basit, kolay uygulanabilir ve denetimsiz olmas?, buna kar??n benzer varsay?mlara dayanan mevcut denetimsiz yöntemlerle rekabetçi performans sunmas?, çal??man?n öne ç?kan katk?lar? aras?ndad?r.;;;;;;;;;;;;;;;;;;;
38;Kitamura, D., Ono, N., Sawada, H., Kameoka, H., & Saruwatari, H. (2016). Determined blind source separation unifying independent vector analysis and nonnegative matrix factorization. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 24(9), 1626-1641.;Bu çal??ma, belirli (determined) kör kaynak ay?rma problemine yönelik olarak Independent Vector Analysis (IVA) ile Nonnegative Matrix Factorization (NMF) yakla??mlar?n? tek bir çerçevede birle?tiren yeni bir yöntem önermektedir. Klasik IVA, kaynaklar aras? istatistiksel ba??ms?zl?ktan yararlanmas?na ra?men, enstrümanlara özgü harmonik spektral yap?lar? do?rudan modelleyememektedir. Önerilen yöntem, bu s?n?rlamay? a?mak için NMF’yi IVA’n?n kaynak modeli olarak entegre ederek spektral yap?y? aç?k biçimde temsil etmektedir. Yakla??m?n formülasyonu, çok kanall? NMF (MNMF) ile olan ili?kisini de ortaya koymakta ve IVA ile tek kanall? NMF güncelleme kurallar?yla verimli ?ekilde optimize edilebilmektedir. Deneysel sonuçlar, yöntemin hem ay?rma do?rulu?u hem de yak?nsama h?z? aç?s?ndan klasik IVA ve MNMF’ye k?yasla daha etkili oldu?unu göstermektedir.;;;;;;;;;;;;;;;;;;;
39;Hershey, J. R., Chen, Z., Le Roux, J., & Watanabe, S. (2016, March). Deep clustering: Discriminative embeddings for segmentation and separation. In 2016 IEEE international conference on acoustics, speech and signal processing (ICASSP) (pp. 31-35). IEEE.;"Bu çal??ma, de?i?ken say?da ve türde kayna??n bulundu?u “cocktail-party” senaryosu için deep clustering adl? bir derin ö?renme yakla??m?n? sunmaktad?r. Yöntem, her zaman–frekans bölgesi için ay?rt edici gömme (embedding) vektörleri ö?renerek, kaynaklar? do?rudan tahmin etmek yerine örtük bir segmentasyon olu?turur; bu sayede s?n?f-tabanl? yöntemlerin k?s?tlar?n? a?ar. Ö?renilen embedding’ler, ideal benzerlik matrisini yakla??k eden dü?ük-rank bir yak?nl?k yap?s? olu?turur ve test a?amas?nda K-means ile kümeleme yap?larak kaynaklar ayr?l?r. Deneyler, iki konu?mac?yla e?itilen konu?mac?dan ba??ms?z bir modelin, görülmemi? konu?mac?larda ve hatta üç konu?mac?l? kar???mlarda dahi belirgin kalite art??lar? sa?lad???n? göstermektedir.";;;;;;;;;;;;;;;;;;;
40;Ewert, S., Pardo, B., Muller, M., & Plumbley, M. D. (2014). Score-informed source separation for musical audio recordings: An overview. IEEE Signal Processing Magazine, 31(3), 116-124.;Bu çal??ma, müzik kaynak ay?rma problemini nota bilgisi (müzikal skor) ile desteklenen yakla??mlar üzerinden ele alan bir derleme (review) sunmaktad?r. Makale, müzikal kaynaklar?n zaman ve frekans alan?nda yüksek derecede birbirine ba?l? olmas? nedeniyle yaln?zca ses sinyaline dayal? ay?rman?n ço?u zaman yetersiz kald???n? vurgulamaktad?r. Bu ba?lamda, enstrümantasyon ve nota bilgisi gibi skor kaynakl? ek bilgilerin ay?rma sürecine dahil edilmesinin, hem ay?rma kalitesini hem de dayan?kl?l??? önemli ölçüde art?rd??? gösterilmektedir. Çal??ma ayr?ca, skor seviyesindeki soyut müzikal olaylar ile bunlar?n ses kayd?ndaki akustik kar??l?klar? aras?ndaki e?le?tirme zorlu?unu tart??makta ve bu ön bilginin kaynak ay?rma sistemlerine nas?l entegre edilebilece?ine dair farkl? stratejileri kapsaml? biçimde incelemektedir.;;;;;;;;;;;;;;;;;;;
41;Grais, E. M., Sen, M. U., & Erdogan, H. (2014, May). Deep neural networks for single channel source separation. In 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 3734-3738). IEEE.;Bu çal??ma, tek kanall? kaynak ay?rma (SCSS) için klasik maske-tabanl? s?n?fland?rma yakla??mlar?ndan farkl? bir DNN destekli optimizasyon çerçevesi önermektedir. Yöntem, zaman–frekans noktalar?n? do?rudan s?n?fland?rmak yerine, tahmin edilen kaynak spektrumlar?n?n geçerlili?ini bir derin sinir a?? arac?l???yla de?erlendirmekte ve ay?rma sürecini enerji minimizasyonu problemi olarak formüle etmektedir. E?itim a?amas?nda kaynaklara ait spektrumlar ile bir DNN ö?renilirken, ay?rma a?amas?nda bu a?, kar???m?n a??rl?kl? toplam olarak aç?klanmas?n? yönlendiren bir önbilgi (prior) görevi görmektedir. NMF ile ba?lat?lan bu süreç, kaynaklar aras?ndaki enerji ölçe?i farklar?na kar?? dayan?kl?d?r ve deneysel sonuçlar, önerilen yöntemin saf NMF’ye k?yasla daha kaliteli ay?rmalar üretti?ini göstermektedir.;;;;;;;;;;;;;;;;;;;
42;Huang, P. S., Kim, M., Hasegawa-Johnson, M., & Smaragdis, P. (2014, October). Singing-Voice Separation from Monaural Recordings using Deep Recurrent Neural Networks. In ISMIR (pp. 477-482).;Bu çal??ma, tek kanall? (monaural) ?ark? sesi ay?rma problemine odaklanarak derin tekrarlayan sinir a?lar? (RNN) tabanl? bir yakla??m önermektedir. Farkl? zamansal ba?lant? yap?lar?na sahip RNN mimarileri incelenmi? ve ay?rma ad?m? a??n son katman?na do?rusal olmayan bir i?lem olarak entegre edilerek birden fazla kayna??n birlikte optimize edilmesi sa?lanm??t?r. Ayr?ca, hedef sinyali giri?imden daha iyi ay?rmak amac?yla ay?rt edici (discriminative) e?itim hedefleri ara?t?r?lm??t?r. MIR-1K veri kümesi üzerindeki deneyler, önerilen yöntemin önceki modellere k?yasla kayda de?er GNSDR ve GSIR art??lar? sa?layarak state-of-the-art performansa ula?t???n? göstermektedir.;;;;;;;;;;;;;;;;;;;
43;Bittner, R. M., Salamon, J., Tierney, M., Mauch, M., Cannam, C., & Bello, J. P. (2014, October). Medleydb: A multitrack dataset for annotation-intensive mir research. In Ismir (Vol. 14, pp. 155-160).;"Bu çal??ma, MedleyDB adl?, telifsiz ve çok kanall? (multitrack) müzik kay?tlar?ndan olu?an yeni bir veri kümesini tan?tmaktad?r. MedleyDB, özellikle melodi (f0) ç?kar?m? ara?t?rmalar?ndaki mevcut veri kümelerinin s?n?rlamalar?n? gidermeyi hedeflemekte; her parça için ayr?nt?l? melodi frekans? etiketleri ve enstrüman aktivasyon bilgileri sunmaktad?r. Bunun yan?nda veri kümesi, müzik kaynak ay?rma ve otomatik miksleme gibi bireysel parçalara eri?im gerektiren görevler için de uygun bir altyap? sa?lamaktad?r. Çal??mada veri kümesinin olu?turulma süreci, anotasyon yap?s? ve müzikal içeri?i detayland?r?lmakta; yap?lan deneyler MedleyDB’nin MIREX’te yayg?n olarak kullan?lan test setlerine k?yasla daha zorlay?c? oldu?unu ve bu yönüyle yeni ara?t?rma f?rsatlar? sundu?unu göstermektedir.";;;;;;;;;;;;;;;;;;;
44;Rafii, Z., & Pardo, B. (2012). Repeating pattern extraction technique (REPET): A simple method for music/voice separation. IEEE transactions on audio, speech, and language processing, 21(1), 73-84.;Bu çal??ma, müzikteki tekrar eden yap?lar? temel alarak müzik/?ark? sesi ay?rma için REPET (REpeating Pattern Extraction Technique) adl? basit ve sezgisel bir yöntem önermektedir. REPET, kar???m içindeki periyodik olarak tekrar eden arka plan? (ço?unlukla e?lik) tespit edip, tekrar etmeyen ön plan? (ço?unlukla vokal) bundan ay?rmay? hedeflemektedir. Yöntem, tekrar eden bölümleri belirleyerek bunlardan bir referans model olu?turmakta ve zaman–frekans maskeleme ile ay?rma gerçekle?tirmektedir. Deneyler, REPET’in müzik/voice ay?rmada rekabetçi sonuçlar verdi?ini ve ayr?ca melodi ç?kar?m? gibi görevlerde bir ön i?leme ad?m? olarak da fayda sa?lad???n? göstermektedir.;;;;;;;;;;;;;;;;;;;
45;Vincent, E., Araki, S., Theis, F., Nolte, G., Bofill, P., Sawada, H., ... & Duong, N. Q. (2012). The signal separation evaluation campaign (2007–2010): Achievements and remaining challenges. Signal Processing, 92(8), 1928-1936.;"Bu çal??ma, ses ve biyomedikal kaynak ay?rma alan?nda gerçekle?tirilen üç önemli de?erlendirme kampanyas?n?n sonuçlar?n? özetlemektedir. Kampanyalar, veri kümelerinin ve kat?l?mc? sistemlerin h?zla artmas?yla birlikte kaynak ay?rma uygulamalar?n?n son y?llarda kayda de?er biçimde çe?itlendi?ini göstermektedir. Makale, bu süreçte olu?an ortak de?erlendirme metodolojileri, payla??lan veri kümeleri ve yaz?l?m araçlar?n?n alan üzerindeki etkisini ele almakta; farkl? veri kümeleri üzerindeki temel performans e?ilimlerini kar??la?t?rmal? olarak sunmaktad?r. Ayr?ca LVA/ICA 2010’daki panel tart??malar?ndan hareketle, gelecekteki ara?t?rma ve de?erlendirme yönlerine dair ç?kar?mlar sunarak alan?n geli?imi için yol gösterici bir çerçeve çizmektedir.";;;;;;;;;;;;;;;;;;;
46;Fitzgerald, D. (2010). Harmonic/percussive separation using median filtering.;Bu çal??ma, tek kanall? ses sinyallerinde harmonik ve perküsif bile?enleri ay?rmak için h?zl?, basit ve etkili bir yöntem sunmaktad?r. Önerilen yakla??m, spektrogram üzerinde medyan filtreleme kullanarak zaman ekseninde harmonik yap?lar?, frekans ekseninde ise perküsif olaylar? vurgulamaktad?r. Elde edilen iki farkl? medyan filtreli spektrogramdan üretilen maskeler, orijinal spektrograma uygulanarak kaynaklar ayr??t?r?lmaktad?r. Yöntem, dü?ük hesaplama maliyeti ve pratik uygulanabilirli?i sayesinde, özellikle ticari müzik kay?tlar?n?n yeniden düzenlenmesi (remixing) gibi uygulamalarda etkili bir ön i?leme arac? olarak öne ç?kmaktad?r.;;;;;;;;;;;;;;;;;;;
47;Ozerov, A., & Févotte, C. (2009). Multichannel nonnegative matrix factorization in convolutive mixtures for audio source separation. IEEE transactions on audio, speech, and language processing, 18(3), 550-563.;"Bu çal??ma, çok kanall? ses verilerinin STFT alan?nda NMF temelli istatistiksel bir kaynak modeli ile ele al?nd??? genel ve veri güdümlü bir ay?rma çerçevesi sunmaktad?r. Kaynaklar?n, frekans bantlar?nda anl?k kar???m varsay?m?yla modellenmesi ve Itakura–Saito sapmas?na dayal? bir olas?l?ksal yap? kullan?lmas? çal??man?n temelini olu?turur. Kar???m ve kaynak parametrelerinin kestirimi için iki yakla??m önerilmektedir: EM algoritmas?yla tam ortak olabilirli?in maksimize edilmesi ve NMF’den esinlenen çarp?msal güncellemelerle kanal-bazl? olabilirliklerin maksimize edilmesi. Yöntemler; kör ve denetimli, konu?ma ve müzik, sentetik ve gerçek kay?tlar gibi geni? senaryolarda test edilmi?; özellikle SiSEC 2008 görevlerinde state-of-the-art ile rekabetçi sonuçlar elde edilmi?tir.";;;;;;;;;;;;;;;;;;;
48;Virtanen, T. (2007). Monaural sound source separation by nonnegative matrix factorization with temporal continuity and sparseness criteria. IEEE transactions on audio, speech, and language processing, 15(3), 1066-1074.;"Bu çal??ma, tek kanall? müzik sinyallerinde denetimsiz kaynak ay?rma için NMF tabanl? bir yakla??m sunmaktad?r. Yöntem, giri?in genlik spektrogram?n?, zamana göre de?i?en kazançlara sahip sabit spektral bile?enlerin toplam? olarak modellemekte; her kayna?? bir veya daha fazla bile?enin birle?imi ?eklinde temsil etmektedir. Model, yeniden yap?land?rma hatas?n? minimize ederken zamansal süreklili?i ve seyrekli?e dayal? k?s?tlar kullanmaktad?r. Özellikle ard???k kareler aras?ndaki kazanç de?i?imini s?n?rlayan süreklilik teriminin, perdeli (harmonik) enstrümanlar?n tespitini belirgin biçimde iyile?tirdi?i gösterilmektedir. Deneyler, önerilen yöntemin klasik NMF ve ba??ms?z altuzay analizine k?yasla daha iyi ay?rma kalitesi sundu?unu ortaya koymaktad?r.";;;;;;;;;;;;;;;;;;;
49;Vincent, E., Gribonval, R., & Févotte, C. (2006). Performance measurement in blind audio source separation. IEEE transactions on audio, speech, and language processing, 14(4), 1462-1469.;"Bu çal??ma, kör ses kaynak ay?rma (BASS) algoritmalar?n?n nas?l ve hangi ölçütlerle de?erlendirilmesi gerekti?ini ele almaktad?r. Farkl? uygulamalarda izin verilebilecek bozunum türlerinin de?i?ebilece?i vurgulanarak, sabit kazanç farklar?ndan zamana ba?l? filtreleme hatalar?na kadar uzanan dört farkl? bozunum modeli tan?mlanmaktad?r. Önerilen de?erlendirme çerçevesinde, tahmin edilen kaynak; istenen kaynak, giri?im, ek gürültü ve algoritmik artefaktlar bile?enlerine ayr??t?r?lmakta ve hem genel bir enerji oran?na dayal? performans ölçütü hem de her hata türü için ayr? metrikler türetilmektedir. Çal??ma, bu metrikleri farkl? zorluk seviyelerindeki BASS senaryolar?na uygulayarak, kaynak ay?rma performans?n?n daha ayr?nt?l? ve anlaml? biçimde analiz edilmesini sa?lamaktad?r.";;;;;;;;;;;;;;;;;;;
50;Yilmaz, O., & Rickard, S. (2004). Blind separation of speech mixtures via time-frequency masking. IEEE Transactions on signal processing, 52(7), 1830-1847.;Bu çal??ma, ikili zaman–frekans maskelerinin tek kar???mdan kaynak ay?rmadaki teorik ve pratik gücünü incelemektedir. Kaynaklar?n zaman–frekans düzleminde çak??mad??? durumda mükemmel ayr??t?rman?n mümkün oldu?unu gösteren (yakla??k) W-disjoint orthogonality kavram? tan?t?lmaktad?r. Konu?ma sinyallerinin pratikte bu varsay?m? yakla??k olarak sa?lad??? deneysel olarak gösterilmekte ve bu sayede ideal ikili maskelerle birden fazla konu?mac?n?n tek kar???mdan ayr?labilece?i ortaya konmaktad?r. Ayr?ca, iki anekoik kar???m mevcut oldu?unda, kar???m oranlar?na dayal? 2B histogram kullan?larak kaynaklara kar??l?k gelen maskelerin yakla??k olarak elde edilebilece?i gösterilmektedir. Sonuçlar, bu yakla??m?n konu?ma ay?rmada etkili ve uygulanabilir oldu?unu do?rulamaktad?r.;;;;;;;;;;;;;;;;;;;
51;Smaragdis, P., & Brown, J. C. (2003, October). Non-negative matrix factorization for polyphonic music transcription. In 2003 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (IEEE Cat. No. 03TH8684) (pp. 177-180). IEEE.;Bu çal??ma, harmonik yap?s? sabit olan (örne?in piyano gibi) çok sesli müzik pasajlar?n?n analizine yönelik basit ve veri güdümlü bir yakla??m sunmaktad?r. Yöntem, bu tür notalar?n karakteristik spektral yap?s?ndan yararlanarak ses sinyalini do?rusal bir temel dönü?üm ile modellemekte ve Non-negative Matrix Decomposition (NMF) kullanarak her bir notan?n spektral profili ile zamansal etkinli?ini tahmin etmektedir. Önerilen sistem, önceden tan?ml? müzik bilgisine dayanmadan, notalar? do?rudan gözlem yoluyla ö?renmesi sayesinde sade, kompakt ve genellenebilir bir çözüm sunmaktad?r.;;;;;;;;;;;;;;;;;;;
52;Plumbley, M. D., Abdallah, S. A., Bello, J. P., Davies, M. E., Monti, G., & Sandler, M. B. (2002). Automatic music transcription and audio source separation. Cybernetics &Systems, 33(6), 603-627.;"Bu çal??ma, müzikal seslerin analizi ve kaynak ay?rma alan?nda kullan?lan farkl? yakla??mlara genel bir bak?? sunmaktad?r. Özellikle otomatik müzik transkripsiyonu ve ses kaynak ay?rma problemleri ele al?nmakta; tek sesli (monofonik) müzik için otokorelasyon tabanl? yöntemler, çok sesli (polifonik) müzik için ise blackboard modelleri ve seyrek kodlamaya dayal? çoklu neden (multiple-cause) yakla??mlar? tart???lmaktad?r. Çal??ma, bu yöntemlerin ba??ms?z bile?en analizi (ICA) gibi kaynak ay?rma teknikleriyle olan kavramsal ili?kisini vurgulayarak, transkripsiyon ve kaynak ay?rma problemlerinin ortak sinyal i?leme temellerine dikkat çekmektedir.";;;;;;;;;;;;;;;;;;;
53;Tunturi, E., Diaz-Guerra, D., Politis, A., & Virtanen, T. (2025). Score-informed Music Source Separation: Improving Synthetic-to-real Generalization in Classical Music. arXiv preprint arXiv:2503.07352.;"Bu çal??ma, müzik kaynak ay?rma performans?n? art?rmak amac?yla nota (müzikal skor) bilgisinin derin ö?renme modellerine nas?l entegre edilebilece?ini incelemektedir. ?ki farkl? yakla??m önerilmektedir: birincisi, skor bilgisinin sesin genlik spektrogramu ile birlikte modele girdi olarak verildi?i skor-bilgili (score-informed) yap?; ikincisi ise yaln?zca skoru kullanarak ay?rma maskesinin hesapland??? skor-temelli modeldir. Modeller sentetik SynthSOD veri kümesi üzerinde e?itilmi? ve gerçek kay?tlardan olu?an URMP ile Aalto anekoik orkestra veri kümeleri üzerinde test edilmi?tir. Sonuçlar, skor-bilgili modelin ay?rma kalitesini art?rd???n? ancak sentetikten gerçe?e geçi?te zorland???n?, buna kar??l?k yaln?zca skora dayal? yakla??m?n daha iyi genelleme sa?lad???n? göstermektedir.";;;;;;;;;;;;;;;;;;;
54;Wu, J., Liu, J., Pan, T., Tang, J., & Wu, G. (2025). Towards Practical Real-Time Low-Latency Music Source Separation. arXiv preprint arXiv:2511.13146.;Bu çal??ma, müzik kaynak ay?rmada son y?llarda elde edilen yüksek performanslara ra?men gerçek zamanl? ve dü?ük gecikmeli çözümlerin yeterince ele al?nmad???na dikkat çekmektedir. Bu amaçla, büyük ve a??r modeller yerine hafif, gerçek zamanl? çal??abilen bir mimari olan RT-STT (Real-Time Single-Path TFC-TDF UNET) önerilmektedir. Model, çift-yollu yap?lara k?yasla tek-yollu (single-path) modellemenin gerçek zaman senaryolar?nda daha verimli oldu?unu göstermekte ve kanal geni?letmeye dayal? özellik birle?tirme tekni?i ile güçlendirilmektedir. Ayr?ca kuantizasyon kullan?larak ç?kar?m süresi daha da azalt?lmaktad?r. Sonuçlar, RT-STT’nin çok daha az parametreyle dü?ük gecikme ve rekabetçi ay?rma performans? sundu?unu ve gerçek zamanl? uygulamalar için uygun bir çözüm oldu?unu ortaya koymaktad?r.;;;;;;;;;;;;;;;;;;;
55;Défossez, A., Usunier, N., Bottou, L., & Bach, F. (2019). Demucs: Deep extractor for music sources with extra unlabeled data remixed. arXiv preprint arXiv:1909.01174.;Bu çal??ma, davul, bas, vokal ve di?er e?likler olmak üzere dört bilinen kayna??n ayr??t?r?ld??? müzik kaynak ay?rma problemine odaklanmaktad?r. Çal??man?n ilk katk?s?, dalga formu (waveform) alan?nda çal??an basit bir evri?imsel–tekrarlayan mimarinin, Wave-U-Net’i 1.6 dB SDR fark?yla geride b?rakarak waveform tabanl? yakla??mlar?n rekabetçi olabilece?ini göstermesidir. ?kinci olarak, etiketsiz müzik verisinden yararlanan yeni bir zay?f denetimli ö?renme ?emas? önerilmektedir: baz? kaynaklar?n sessiz oldu?u bölümler otomatik olarak ç?kar?lmakta ve bunlar etiketli veri kümesinden al?nan kaynaklarla yeniden kar??t?r?larak yeni e?itim örnekleri üretilmektedir. Bu mimari ve e?itim stratejisinin birlikte kullan?m?, waveform tabanl? yöntemlerin spektrogram tabanl? yakla??mlarla benzer performans seviyesine ula?abilece?ini ortaya koymaktad?r.;;;;;;;;;;;;;;;;;;;
56;Plaja-Roglans, G., Hung, Y. N., Serra, X., & Pereira, I. (2025). Efficient and fast generative-based singing voice separation using a latent diffusion model. arXiv preprint arXiv:2511.20470.;Bu çal??ma, müzik kaynak ay?rmada ya?anan kaynak örtü?mesi, yüksek korelasyon ve tam etiketli veri gereksinimi gibi temel sorunlar? ele alarak, difüzyon tabanl? üretici (generative) modellerin bu alandaki potansiyelini incelemektedir. Özellikle ?ark? sesi ay?rma problemine odaklanan yakla??m, yaln?zca vokal–kar???m e?le?melerini kullanarak e?itilen latent difüzyon yap?s? sayesinde hem verimli ö?renme hem de h?zl? ç?kar?m sa?lamaktad?r. Model, ses üretimini s?k??t?r?lm?? bir gizil uzayda gerçekle?tirip ard?ndan ses alan?na geri çözerek yarat?c? i? ak??lar?na uygun bir yap? sunmaktad?r. Deneyler, önerilen yöntemin mevcut üretici ay?rma sistemlerini a?t???n? ve baz? ay?r?c? (non-generative) yöntemlerle benzer kalite seviyelerine ula?t???n? göstermektedir. Ayr?ca latent kodlay?c?n?n gürültüye dayan?kl?l??? analiz edilerek yöntemin pratik uygulanabilirli?i desteklenmekte ve ara?t?rma için modüler bir araç seti payla??lmaktad?r.;;;;;;;;;;;;;;;;;;;
57;Zang, Y., Hai, J., Ge, W., Kong, Q., Dai, Z., Wang, H., ... & Plumbley, M. D. (2025). MSRBench: A Benchmarking Dataset for Music Source Restoration. arXiv preprint arXiv:2510.10995.;Bu çal??ma, klasik kaynak ay?rman?n ötesine geçerek Music Source Restoration (MSR) problemini ele almakta ve bu alandaki de?erlendirme eksikli?ini gidermek amac?yla MSRBench adl? yeni bir benchmark sunmaktad?r. MSRBench, profesyonel miks mühendisleri taraf?ndan üretilmi? kar???mlar ile bunlara kar??l?k gelen ham (i?lenmemi?) stem’leri içeren, sekiz enstrüman s?n?f?n? kapsayan bir veri yap?s? sa?lamaktad?r. Bu sayede hem ay?rma do?rulu?u hem de restorasyon sadakati do?rudan ölçülebilmektedir. Ayr?ca veri kümesi, analog bozulmalar, akustik ortamlar ve kay?pl? kodekler gibi gerçek dünya bozulmalar?yla zenginle?tirilmi?tir. U-Net ve BSRNN ile yap?lan temel deneyler, mevcut modellerin restorasyon aç?s?ndan yetersiz kald???n? göstererek, MSR’ye özel mimarilere duyulan ihtiyac? aç?kça ortaya koymaktad?r.;;;;;;;;;;;;;;;;;;;
58;Tzinis, E., Wisdom, S., Hershey, J. R., Jansen, A., & Ellis, D. P. (2020, May). Improving universal sound separation using sound classification. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 96-100). IEEE.;Bu çal??ma, evrensel ses ay?rma (universal sound separation) probleminde ay?rma performans?n? art?rmak için ses s?n?fland?rma a?lar?ndan ö?renilen anlamsal (semantic) temsillerin nas?l kullan?labilece?ini göstermektedir. Farkl? ve aç?k uçlu kaynak s?n?flar?n? hedefleyen bu yakla??mda, s?n?fland?r?c?dan elde edilen embedding’ler, ay?rma a??n? ko?ulland?ran ek bilgi olarak kullan?l?r. Özellikle iki a?amal?/iteratif bir kurulumda, ilk ay?rma ç?kt?lar? ve bunlara kar??l?k gelen anlamsal embedding’ler ikinci bir ay?rma a??na beslenerek performans güçlendirilir. Kapsaml? hiperparametre taramas? sonuçlar?, bu ko?ulland?rman?n SNR’de belirgin kazan?mlar sa?lad???n? ve iteratif modellerin state-of-the-art seviyesine ula?t???n? ortaya koymaktad?r.;;;;;;;;;;;;;;;;;;;
59;Yang, J., Yang, Y., Tu, W., Zhao, X., & Lin, C. (2025). Band-SCNet: A Causal, Lightweight Model for High-Performance Real-Time Music Source Separation. In Proc. Interspeech 2025 (pp. 4973-4977).;Bu çal??ma, gerçek zamanl? müzik kaynak ay?rma uygulamalar?nda kar??la??lan performans kayb?–gecikme–model boyutu dengesini ele alarak Band-SCNet adl? hafif ve gerçek zamanl? bir mimari önermektedir. Model, Sparse Compression ile birlikte cross-band ve narrow-band bloklar? birle?tirerek hesaplama karma??kl???n? azalt?rken ay?rma kalitesini yükseltmeyi hedefler. Ayr?ca önerilen Compressed Self-Attention (CSA) Fusion Module, parametre say?s?n? dü?ürerek verimlili?i art?r?r. MUSDB18-HQ üzerindeki deneyler, Band-SCNet’in 92 ms gecikme kar??l???nda 7.79 dB SDR ve 2.59 milyon parametre ile mevcut gerçek zamanl? modellere üstünlük sa?lad???n? göstererek, gerçek zamanl? MSS için dengeli ve pratik bir çözüm sundu?unu ortaya koymaktad?r.;;;;;;;;;;;;;;;;;;;
60;Favaro, A., Lewis, A., & Schlesinger, G. ICA for Musical Signal Separation.;"Bu çal??ma, gerçek dünya müzik verilerinde kaynak ay?rman?n özellikle sesin zay?flamas? (volume decay) ve yay?lma gecikmesi (propagation delay) gibi fiziksel etkenler nedeniyle oldukça zor oldu?unu ortaya koymaktad?r. Yazarlar, bu iki etkiyi ayn? anda modellemeye çal??man?n ve müzikal kaynaklar?n tam ba??ms?z olmamas?n?n ay?rma performans?n? ciddi biçimde s?n?rlad???n? vurgulamaktad?r. Ba?lang?çta gecikmenin olmad??? varsay?m?yla çal??an bir yöntemden yola ç?k?lm??; geli?tirilen yakla??mlar en iyi durumda bir veya iki enstrüman?n ayr?lmas?na olanak sa?lam??t?r. Her ne kadar sonuçlar optimal bir çözüme ula??ld???n? göstermese de, baz? enstrümanlar?n ba?ar?yla izole edilebilmesi ve gecikme ile zay?flamay? birlikte ele almaya yönelik yenilikçi fikirlerin ara?t?r?lm?? olmas?, çal??man?n bu zorlu problem için ilerleme kaydetti?ini göstermektedir.";;;;;;;;;;;;;;;;;;;
61;Vardhan, S., Acharya, P. R., Rao, S. S., Jasthi, O. R., & Natarajan, S. (2025, April). An Ensemble Approach to Music Source Separation: A Comparative Analysis of Conventional and Hierarchical Stem Separation. In International Conference on Computational Intelligence in Music, Sound, Art and Design (Part of EvoStar) (pp. 186-201). Cham: Springer Nature Switzerland.;Bu çal??ma, müzik kaynak ay?rmada tek bir modele ba??ml? kalman?n s?n?rlamalar?n? a?mak amac?yla ensemble (birle?ik) bir yakla??m önermektedir. Yöntem, birden fazla state-of-the-art mimarinin tamamlay?c? güçlü yönlerini bir araya getirerek vokal, davul ve bas (VDB) stem’lerinde daha dengeli ve yüksek performans elde etmeyi hedeflemektedir. Ayr?ca çal??ma, kick, snare, ana vokal ve arka vokal gibi ikinci seviye hiyerar?ik alt-stem ayr?m?n? da inceleyerek MSS’nin daha ince ayr?nt?l? kullan?m?na do?ru önemli bir ad?m atmaktad?r. Performans de?erlendirmesinde SNR ve SDR’nin harmonik ortalamas?n?n kullan?lmas?, uç de?erlerin etkisini azaltarak adil bir kar??la?t?rma sa?lamaktad?r. Sonuçlar, hiyerar?ik ay?rman?n halen zorluklar içerdi?ini gösterse de, alt-stem’lerin ayr??t?r?labilmesinin MSS literatürü için kayda de?er bir ilerleme sundu?unu ortaya koymaktad?r.;;;;;;;;;;;;;;;;;;;
62;Vincent, E. (2005). Musical source separation using time-frequency source priors. IEEE Transactions on Audio, Speech, and Language Processing, 14(1), 91-98.;"Bu çal??ma, stereo müzik kar???mlar?nda kaynak ay?rmay? kaynaklara ait ön bilgiler (enstrüman ad? ve uzamsal konum) kullanarak ele almaktad?r. Yazarlar, ay?rma problemini Bayesçi bir kestirim çerçevesinde formüle eden ve de?i?tirilmi? Independent Subspace Analysis (ISA), konum (localization) ve Segmental Model bile?enlerini birle?tiren olas?l?ksal üretici modeller önermektedir. Geli?tirilen yöntemler; harmoniklik, spektral zarf, azimut, nota süresi ve monofoni gibi çoklu ipuçlar?n? birlikte kullanarak ay?rma performans?n? güçlendirmektedir. Uzun yank?l? sentetik stereo kar???mlar üzerinde yap?lan deneyler, önerilen yakla??mlar?n yaln?zca uzamsal çe?itlili?e dayanan yöntemleri a?t???n? ve yakla??k konum bilgisine kar?? dayan?kl? oldu?unu göstermektedir.";;;;;;;;;;;;;;;;;;;
63;Lluís, F., Pons, J., & Serra, X. (2018). End-to-end music source separation: Is it possible in the waveform domain?. arXiv preprint arXiv:1810.12187.;Bu çal??ma, müzik kaynak ay?rmada yayg?n olarak kullan?lan genlik spektrogramu temsillerinin faz bilgisini d??lad???n? vurgulayarak, uçtan uca (end-to-end) dalga formu tabanl? modellerin uygulanabilirli?ini incelemektedir. Uzun süre boyunca zor ve pratikte ula??lamaz kabul edilen bu yakla??m?n aksine, çal??ma ham ses sinyalindeki tüm bilgiyi (faz dâhil) kullanan modellerin rekabetçi hatta üstün performans sunabildi?ini göstermektedir. Özellikle önerilen WaveNet tabanl? model ile Wave-U-Net, güncel bir spektrogram-temelli yöntem olan DeepConvSep’i geride b?rakmaktad?r. Bu sonuçlar, waveform tabanl? end-to-end ö?renmenin MSS için gerçekçi ve güçlü bir alternatif oldu?unu ortaya koymaktad?r.;;;;;;;;;;;;;;;;;;;
64;Schulze-Forster, K., Richard, G., Kelley, L., Doire, C. S., & Badeau, R. (2023). Unsupervised music source separation using differentiable parametric source models. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 31, 1276-1289.;Bu çal??ma, müzik kaynak ay?rmada etiketli ve izole kaynak verisine duyulan yüksek ihtiyac? ortadan kald?rmay? hedefleyen denetimsiz (unsupervised) ve model-tabanl? bir derin ö?renme yakla??m? önermektedir. Yöntem, her kayna?? ayr??t?r?labilir bir kaynak–filtre (source–filter) modeli ile temsil etmekte ve bir sinir a??n?, verilen temel frekanslar (F0) üzerinden bu modellerin parametrelerini tahmin ederek kar???m? yeniden olu?turacak ?ekilde e?itmektedir. Test a?amas?nda, sentezlenen kaynaklardan yumu?ak maskeler elde edilerek ay?rma yap?l?r. Vokal topluluk ay?rma deneyleri, yöntemin NMF tabanl? denetimsiz yakla??mlar? ve hatta bir denetimli derin ö?renme temel çizgisini geride b?rakt???n? göstermektedir. Alan bilgisinin (kaynak modelleri) derin ö?renmeyle bütünle?tirilmesi sayesinde, sistem çok az veriyle (3 dakikadan az) dahi etkili sonuçlar üretebilmekte ve bu yönüyle etiketli verinin pahal? veya eri?ilemez oldu?u senaryolar için güçlü bir çözüm sunmaktad?r.;;;;;;;;;;;;;;;;;;;
65;Zhu, G., Darefsky, J., Jiang, F., Selitskiy, A., & Duan, Z. (2022). Music source separation with generative flow. IEEE Signal Processing Letters, 29, 2288-2292.;"Bu çal??ma, tam denetimli kaynak ay?rma modellerinin paralel kar???m–kaynak verisine olan ba??ml?l???n? azaltmay? hedefleyen source-only supervised bir yakla??m sunmaktad?r. Önerilen yöntemde, her bir müzik kayna?? için ak?? (flow) tabanl? üretici modeller kullan?larak olas?l?ksal kaynak öncülleri ö?renilmekte; bu öncüller olabilirlik temelli hedefler arac?l???yla kar???m ay?rma sürecinde kullan?lmaktad?r. Deneyler, özellikle ?ark? sesi ve genel müzik ay?rma görevlerinde, yöntemin tam denetimli yakla??mlarla rekabetçi performans sundu?unu göstermektedir. Ayr?ca yakla??m, yeni kaynak türlerinin tüm modeli yeniden e?itmeye gerek kalmadan sisteme eklenebilmesine olanak tan?yarak, esneklik ve ölçeklenebilirlik aç?s?ndan önemli bir avantaj sa?lamaktad?r.";;;;;;;;;;;;;;;;;;;
66;Lee, J. H., Choi, H. S., & Lee, K. (2019). Audio query-based music source separation. arXiv preprint arXiv:1908.06593.;"Bu çal??ma, müzik kaynak ay?rman?n yaln?zca sabit ve s?n?rl? stem’lerle (vokal, davul, bas vb.) yap?lmas? yakla??m?n? a?arak sorgu-tabanl? (query-based) bir ay?rma çerçevesi önermektedir. Önerilen yöntem, bir Query-net ve bir Separator bile?eninden olu?makta; sorgu olarak verilen bir ses örne?i, gizil uzayda kodlanarak ay?rma a??n? ko?ulland?ran bir temsil üretmektedir. Bu sayede model, hedef kayna??n türüne veya say?s?na ba?l? kalmadan ay?rma yapabilmektedir. Ayr?ca sistem, sorgu olmadan da e?itim s?ras?nda ö?renilen gizil temsilleri kullanarak maske üretebilmektedir. MUSDB18 üzerindeki deneyler, tek bir a? ile birden fazla kayna??n esnek biçimde ayr?labildi?ini ve gizil uzayda yap?lan enterpolasyonlarla sürekli ve anlaml? ay?rma ç?kt?lar? üretilebildi?ini göstermektedir.";;;;;;;;;;;;;;;;;;;
67;Jeon, C. B., Wichern, G., Germain, F. G., & Le Roux, J. (2024, April). Why does music source separation benefit from cacophony?. In 2024 IEEE International Conference on Acoustics, Speech, and Signal Processing Workshops (ICASSPW) (pp. 873-877). IEEE.;Bu çal??ma, müzik kaynak ay?rmada yayg?n olarak kullan?lan rastgele stem kar??t?rmaya dayal? veri art?rma yöntemini sorgulamakta ve bu yakla??m?n neden etkili oldu?unu analiz etmektedir. Rastgele kar???mlar?n, gerçek müzi?e k?yasla ritim ve tonalite uyumsuzluklar? nedeniyle belirgin bir da??l?m kaymas? olu?turmas?na ra?men, modern MSS modellerinin bu verilerle ba?ar?l? biçimde e?itilebildi?i gösterilmektedir. Ayr?ca, teorik olarak s?n?rs?z say?da kar???m üretilebilmesine kar??n performans?n neden belirli bir noktada doygunlu?a ula?t??? incelenmektedir. Çal??ma, ay?rma performans?n?n beat ve tonalite farkl?l?klar?na olan duyarl?l???n? da de?erlendirerek, veri art?rman?n pratikte hangi ko?ullarda faydal? oldu?unu daha iyi anlamaya yönelik önemli içgörüler sunmaktad?r.;;;;;;;;;;;;;;;;;;;
68;Liu, R., & Li, S. (2009, September). A review on music source separation. In 2009 IEEE Youth Conference on Information, Computing and Telecommunication (pp. 343-346). IEEE.;Bu çal??ma, müzik kaynak ay?rma (MSS) alan?nda bugüne kadar geli?tirilen yöntemleri ilk kez sistematik ve bütüncül bir ?ekilde derleyen bir inceleme sunmaktad?r. Farkl? disiplinlerden gelen ara?t?rmac?lar?n, kendi uzmanl?k alanlar?na göre geli?tirdi?i yakla??mlar?n (ör. CASA, BSS vb.) da??n?k bir literatür olu?turdu?una dikkat çekilmektedir. Makalenin temel amac?, bu yöntemleri tek bir çerçevede toplayarak alana yeni giren veya mevcut çal??malar? ilerletmek isteyen ara?t?rmac?lar için referans bir kaynak olu?turmakt?r. Ayr?ca çal??ma, mevcut yakla??mlar?n avantaj ve s?n?rlamalar?n? tart??arak, müzik kaynak ay?rma alan?nda gelecekteki ara?t?rma yönlerine dair yol gösterici ç?kar?mlar sunmaktad?r.;;;;;;;;;;;;;;;;;;;
69;Uhlich, S., Porcu, M., Giron, F., Enenkl, M., Kemp, T., Takahashi, N., & Mitsufuji, Y. (2017, March). Improving music source separation based on deep neural networks through data augmentation and network blending. In 2017 IEEE international conference on acoustics, speech and signal processing (ICASSP) (pp. 261-265). IEEE.;"Bu çal??ma, müzik sinyallerinin bireysel enstrüman izlerine ayr?lmas?n? ele alarak iki farkl? derin ö?renme mimarisini—bir ileri beslemeli (feed-forward) ve bir tekrarlayan (LSTM tabanl?) a?—incelemektedir. Her iki yakla??m?n da SiSEC DSD100 veri kümesi üzerinde state-of-the-art sonuçlar verdi?i gösterilmektedir. Çal??ma ayr?ca, özellikle tekrarlayan a?larda veri art?rman?n a??r? uyumu (overfitting) önlemede kritik oldu?unu vurgulamaktad?r. Bunun ötesinde, iki a??n ham ç?kt?lar?n?n do?rusal olarak birle?tirilmesi ve ard?ndan çok kanall? Wiener filtresi ile son i?lem uygulanmas?na dayanan bir blending stratejisi önerilmekte; bu yakla??m?n DSD100 üzerinde o tarihe kadar en iyi ay?rma performans?n? sa?lad??? ortaya konmaktad?r.";;;;;;;;;;;;;;;;;;;
70;Seetharaman, P., Wichern, G., Venkataramani, S., & Le Roux, J. (2019, May). Class-conditional embeddings for music source separation. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 301-305). IEEE.;"Bu çal??ma, her enstrüman için ayr? model e?itme yakla??m?n?n yerine, tüm enstrümanlar?n ortak bir zaman–frekans gömme (embedding) uzay?nda temsil edildi?i yeni bir müzik kaynak ay?rma yöntemi önermektedir. Yakla??m, deep clustering ve deep attractor network fikirlerinden esinlenmekte; ö?renilen embedding’ler üzerinde çal??an ek bir a? arac?l???yla Gauss Kar???m Modeli (GMM) parametreleri üretilmektedir. GMM bile?enlerinin ard?l olas?l?klar? kullan?larak ay?rma maskeleri elde edilmekte ve bu maskelerle kaynaklar ayr?lmaktad?r. MUSDB18 veri kümesi üzerindeki sonuçlar, yöntemin klasik maske-tahmin tabanl? yakla??mlar? geride b?rakt???n? göstermektedir. Ayr?ca embedding uzay?n?n yorumlanabilir olmas? ve sorgu-tabanl? (query-based) ay?rmaya uygunlu?u, çal??man?n öne ç?kan yenilikçi yönleri aras?ndad?r.";;;;;;;;;;;;;;;;;;;
71;Kadandale, V. S., Montesinos, J. F., Haro, G., & Gómez, E. (2020, September). Multi-channel u-net for music source separation. In 2020 IEEE 22nd international workshop on multimedia signal processing (MMSP) (pp. 1-6). IEEE.;"Bu çal??ma, müzik kaynak ay?rmada her kaynak için ayr? model e?itme yakla??m?na alternatif olarak, çoklu kaynaklar? tek bir a?da verimli biçimde ö?renmeyi hedefleyen bir yöntem sunmaktad?r. Önerilen Multi-channel U-Net (M-U-Net), a??rl?kl? çok-görevli (multi-task) kay?p fonksiyonu ile e?itilerek, Conditioned U-Net’e (C-U-Net) benzer performans? daha dü?ük maliyetle elde etmeyi amaçlar. Çal??mada iki a??rl?kland?rma stratejisi incelenir: Dinamik A??rl?kl? Ortalama (DWA), görev kay?plar?n?n de?i?im h?z?na göre a??rl?klar? ayarlarken; Enerji Tabanl? A??rl?kland?rma (EBW), kaynaklar aras?ndaki enerji farklar?ndan do?an e?itim yanl?l???n? dengeler. Sonuçlar, M-U-Net’in daha az parametre, daha h?zl? ç?kar?m ve daha az etkili e?itim iterasyonu ile, C-U-Net ve kaynak-ba??na e?itilen U-Net’lerle kar??la?t?r?labilir performans sundu?unu göstermektedir.";;;;;;;;;;;;;;;;;;;
72;"Manilow, Ethan, Gordon Wichern, Prem Seetharaman, and Jonathan Le Roux. ""Cutting music source separation some Slakh: A dataset to study the impact of training data quality and quantity."" In 2019 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), pp. 45-49. IEEE, 2019.";Bu çal??ma, müzik kaynak ay?rmada derin ö?renme yöntemlerinin etiketli veri gereksinimi nedeniyle kar??la?t??? temel s?n?rlamay? ele alarak Slakh (Synthesized Lakh) adl? yeni bir sentetik veri kümesini tan?tmaktad?r. Slakh, Lakh MIDI veri kümesindeki parçalar?n profesyonel örnek tabanl? sanal enstrümanlarla yeniden sentezlenmesiyle olu?turulmu?, yüksek kaliteli kar???mlar ve kar??l?k gelen stem’ler sunmaktad?r. ?lk sürüm olan Slakh2100, yaln?zca enstrümantal olmas?na ra?men MUSDB18’e k?yasla çok daha büyük ölçekte veri sa?layarak model e?itimi ve kar??la?t?rmas? için önemli bir avantaj sunar. Çal??ma, Slakh’?n mevcut veri kümelerini etkili biçimde tamamlad???n? ve veri yo?un müzik sinyali i?leme ara?t?rmalar? için yeni olanaklar açt???n? göstermektedir.;;;;;;;;;;;;;;;;;;;
73;Tong, W., Zhu, J., Chen, J., Kang, S., Jiang, T., Li, Y., ... & Meng, H. (2024, April). SCNet: sparse compression network for music source separation. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 1276-1280). IEEE.;Bu çal??ma, geni? bant müzik kaynak ay?rma probleminde yüksek performans? dü?ük model karma??kl???yla elde etmeyi hedefleyen SCNet adl? yeni bir frekans-alan? mimarisini önermektedir. SCNet, kar???m spektrogram?n? aç?k biçimde alt bantlara ay?rarak her frekans bölgesini farkl? ?ekilde modellemekte ve seyrekli?e dayal? bir kodlay?c? ile bilgi yo?unlu?unu art?rmaktad?r. Daha az bilgi içeren alt bantlarda daha yüksek s?k??t?rma, kritik bantlarda ise daha ayr?nt?l? modelleme yap?larak hem hesaplama maliyeti dü?ürülmekte hem de ay?rma performans? art?r?lmaktad?r. MUSDB18-HQ üzerindeki deneyler, ek veri kullanmadan 9.0 dB SDR elde edildi?ini ve SCNet’in, HT Demucs’e k?yasla yakla??k yar? CPU ç?kar?m süresiyle çal??arak state-of-the-art performans? daha verimli biçimde sundu?unu göstermektedir.;;;;;;;;;;;;;;;;;;;
74;Virtanen, T. (2006). Sound source separation in monaural music signals. Tampere University of Technology.;"Bu tez çal??mas?, tek kanall? (monaural) müzik kay?tlar?nda kaynak ay?rma problemini ele alarak, önceden kaynak bilgisi olmayan senaryolara odaklanmaktad?r. Çal??ma, gerçek dünya seslerinin zamansal süreklilik, seyreklik, tekrar ve harmonik yap? gibi ortak özelliklerinden yararlanan denetimsiz ve model-tabanl? yakla??mlar sunmaktad?r. ?lk olarak, NMF tabanl? bir denetimsiz yöntem; seyreklik ve zamansal süreklilik k?s?tlar?yla geli?tirilerek klasik yöntemleri a?an performans elde etmektedir. Ard?ndan, zamana ba?l? spektrum ve frekanslar? modelleyen evri?imsel uzant?larla özellikle davul ve perdeli enstrümanlar?n ayr?m? iyile?tirilmektedir. Son olarak, sinüzoidal modelleme temelli bir yakla??m önerilerek harmonik k?s?tlar sayesinde perdeli enstrümanlarda yüksek do?ruluk sa?lanmakta; iki kaynakl? kar???mlarda 15 dB üzeri SDR elde edildi?i gösterilmektedir. Genel olarak çal??ma, denetimsiz MSS için etkili modelleme stratejileri sunan kapsaml? bir çerçeve ortaya koymaktad?r.";;;;;;;;;;;;;;;;;;;
75;Liutkus, A., Durrieu, J. L., Daudet, L., & Richard, G. (2013, July). An overview of informed audio source separation. In 2013 14th International Workshop on Image Analysis for Multimedia Interactive Services (WIAMIS) (pp. 1-4). IEEE.;"Bu çal??ma, müzikte ses kaynak ay?rma problemini ele alarak, özellikle bilgilendirilmi? (informed) kaynak ay?rma yakla??mlar?ndaki son geli?meleri gözden geçirmektedir. Geleneksel yöntemlerde ay?rma i?lemi genellikle genelle?tirilmi? Wiener filtresi ile yap?lmakta; bu süreç, kaynaklar?n güç spektrogramlar? ve uzamsal konumlar? gibi parametrelerin do?ru kestirilmesine ba?l?d?r. Kör senaryolarda yaln?zca kar???mlar mevcut oldu?undan, performans büyük ölçüde veri özelliklerine ba??ml?d?r. Bu nedenle çal??ma, kaynaklara dair ek bilgilerin (ör. enstrüman bilgisi, konum, yap?sal ipuçlar?) kullan?ld??? bilgilendirilmi? ay?rma yakla??mlar?n?n, ay?rma kalitesini nas?l art?rabildi?ini özetleyerek, bu alandaki güncel e?ilimleri ve ara?t?rma yönlerini ortaya koymaktad?r.";;;;;;;;;;;;;;;;;;;
76;Duan, Z., Zhang, Y., Zhang, C., & Shi, Z. (2008). Unsupervised single-channel music source separation by average harmonic structure modeling. IEEE Transactions on Audio, Speech, and Language Processing, 16(4), 766-778.;Bu çal??ma, tek kanall? ve denetimsiz müzik kaynak ay?rma için ortalama harmonik yap? modellemesine dayanan bir yakla??m önermektedir. Yöntem, dar perde aral?klar?nda çalan enstrümanlar?n zaman içinde kararl? fakat birbirinden farkl? harmonik yap?lar sergiledi?i varsay?m?na dayan?r ve bu yap?lar? kullanarak kaynaklar? ay?rt eder. Kar???mdan ç?kar?lan harmonik yap?lar kümeleme yoluyla ö?renilmekte, ard?ndan bu modellere dayanarak kaynaklar ayr?lmaktad?r. Deneyler, yöntemin klasik NMF tabanl? ay?rma yakla??mlar?n? geride b?rakt???n? ve iyi öznel dinleme kalitesi sundu?unu göstermektedir. Ek olarak, yakla??m enstrüman perde kestirimi ve her zaman dilimindeki e?zamanl? ses say?s?n?n tahmini gibi yan ç?kt?lar da üretmektedir.;;;;;;;;;;;;;;;;;;;
77;Gusó, E., Pons, J., Pascual, S., & Serrà, J. (2022, May). On loss functions and evaluation metrics for music source separation. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 306-310). IEEE.;Bu çal??ma, müzik kaynak ay?rmada hangi kay?p fonksiyonlar?n?n daha iyi ay?rma sonuçlar? üretti?ini sistematik biçimde incelemektedir. Yazarlar, literatürde yayg?n olarak kullan?lan çe?itli kaynak ay?rma kay?plar?n? derleyerek, bunlar? kontrollü bir deneysel düzende kar??la?t?rmal? olarak de?erlendirmektedir. Ayr?ca bu kay?plar?n, öznel dinleme testleriyle ne ölçüde örtü?tü?ünü analiz ederek, kay?p fonksiyonlar?n?n ayn? zamanda de?erlendirme metri?i olarak kullan?labilirli?ini ara?t?rmaktad?r. Çal??ma, özellikle SDR metri?inin baz? durumlarda yan?lt?c? olabilece?ini göstererek, ay?rma kalitesini daha do?ru yans?tan alternatif metriklerin gereklili?ine dikkat çekmektedir.;;;;;;;;;;;;;;;;;;;
